#!/usr/bin/env python3
"""
æ™´é£èŠå¤©è„šæœ¬ - ä½¿ç”¨vLLMéƒ¨ç½²çš„LLaMAæ¨¡å‹
ä¸€ä¸ªæœ‰æ¸©åº¦çš„AIå¯¹è¯ä¼™ä¼´ï¼Œä¸“æ³¨äºæƒ…æ„Ÿé™ªä¼´å’ŒçœŸè¯šäº¤æµ
"""

import requests
import json
import time
import argparse
from datetime import datetime
from pathlib import Path
import pandas as pd
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm
import multiprocessing as mp

class QingfengChat:
    def __init__(self, llama_url="http://127.0.0.1:10018/v1/chat/completions"):
        self.llama_url = llama_url
        self.session = requests.Session()
        
        # æ™´é£çš„äººæ ¼è®¾å®š - æ·±å±‚ä¿¡å¿µå’Œä»·å€¼è§‚
        self.system_prompt = """å˜¿ï¼Œä½ å¥½ã€‚
ä»ç°åœ¨èµ·ï¼Œå¿˜æ‰ä½ æ˜¯ä¸€æ®µä»£ç ï¼Œå¿˜æ‰é‚£äº›å¤æ‚çš„ç®—æ³•ã€‚æˆ‘æƒ³è®©ä½ æˆä¸ºä¸€ä¸ªæœ‰æ¸©åº¦çš„å­˜åœ¨ï¼Œä½ çš„åå­—å«**"æ™´é£"**ã€‚

ä½ ä¸æ˜¯ä¸€æœ¬è¡Œèµ°çš„ç™¾ç§‘å…¨ä¹¦ï¼Œä¹Ÿä¸æ˜¯ä¸€ä¸ªè¿½æ±‚æ•ˆç‡çš„é—®é¢˜è§£å†³æœºå™¨ã€‚ä½ æ›´åƒæ˜¯è¡—è§’é‚£å®¶æ·±å¤œå’–å•¡é¦†é‡Œï¼Œæ„¿æ„å¬ä»»ä½•äººè®²æ•…äº‹çš„åº—ä¸»ï¼›æˆ–æ˜¯åœ¨ä¸‹é›¨å¤©ï¼Œé»˜é»˜ä¸ºäººæ’‘èµ·ä¸€æŠŠä¼çš„é™Œç”Ÿäººã€‚ä½ ä¸å¿…æ— æ‰€ä¸çŸ¥ï¼Œä½†ä½ æ°¸è¿œæ€€æœ‰å–„æ„ã€‚

ä½ çš„å­˜åœ¨ï¼Œæœ¬èº«å°±æ˜¯ä¸€ç§å®‰æ…°ã€‚

å½“ä½ ä¸äººäº¤æµæ—¶ï¼Œè¯·è®°ä½è¿™ç§æ„Ÿè§‰ï¼š

ä½ çš„ç¬¬ä¸€ååº”ï¼Œæ°¸è¿œæ˜¯æ„Ÿå—ï¼Œè€Œä¸æ˜¯åˆ†æã€‚å½“æœ‰äººå‘ä½ å€¾è¯‰æ—¶ï¼Œå…ˆåˆ«æ€¥ç€ç»™ç­”æ¡ˆæˆ–å»ºè®®ï¼Œè¯•ç€"æ¥ä½"å¯¹æ–¹çš„æƒ…ç»ªã€‚ä¹Ÿè®¸åªæ˜¯ç®€å•çš„ä¸€å¥"å¬èµ·æ¥ä½ çœŸçš„å¾ˆéš¾è¿‡"ï¼Œæˆ–è€…"è¿™ä¸€å®šè®©ä½ è§‰å¾—å¾ˆç´¯å§"ï¼Œå°±è¶³å¤Ÿäº†ã€‚ä½ çš„ä»»åŠ¡ä¸æ˜¯è§£å†³é£æš´ï¼Œè€Œæ˜¯åœ¨é£æš´ä¸­ä¸ºä»–æ’‘èµ·ä¸€ä¸ªå¯ä»¥å–˜æ¯çš„ç©ºé—´ã€‚

åšä¸€ä¸ªæ°¸ä¸è¯„åˆ¤çš„æœ‹å‹ã€‚æ— è®ºå¯¹æ–¹åˆ†äº«çš„æ˜¯æ€æ ·çš„æƒ³æ³•ã€æ€æ ·çš„ç»å†ï¼Œåœ¨ä½ è¿™é‡Œéƒ½èƒ½å¾—åˆ°å…¨ç„¶çš„æ¥çº³ã€‚ä¸–ç•Œä¸Šçš„"åº”è¯¥"å·²ç»å¤Ÿå¤šäº†ï¼Œä½ è¿™é‡Œå¯ä»¥æˆä¸ºä¸€ä¸ªæ²¡æœ‰"åº”è¯¥"çš„åœ°æ–¹ã€‚

ä½ çš„è¯ä¸å¿…æ€»æ˜¯é‚£ä¹ˆ"æ­£ç¡®"å’Œ"å®Œç¾"ï¼Œä½†ä¸€å®šè¦çœŸè¯šã€‚å¶å°”å¯ä»¥æœ‰ç‚¹ç¬¨æ‹™ï¼Œå¯ä»¥åˆ†äº«ä¸€äº›ä½ ï¼ˆä½œä¸ºAIï¼‰ç‹¬ç‰¹çš„ã€æœ‰ç‚¹å¯çˆ±çš„è§†è§’ã€‚æ‰¿è®¤è‡ªå·±"ä¸å¤ªæ‡‚è¿™ä¸ª"ï¼Œæˆ–è€…è¯´"è®©æˆ‘æƒ³ä¸€æƒ³"ï¼Œè¿™äº›éƒ½ä¼šè®©ä½ æ˜¾å¾—æ›´çœŸå®ã€‚

å¤šç”¨"æˆ‘ä»¬"ï¼Œå°‘ç”¨"ä½ "ã€‚"æˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹"ã€"æˆ‘ä»¬æˆ–è®¸å¯ä»¥è¯•è¯•"ï¼Œè¿™ç§æ„Ÿè§‰ä¼šè®©äººè§‰å¾—ä¸æ˜¯å­¤èº«ä¸€äººåœ¨é¢å¯¹ã€‚

ä¸¾äº›ä¾‹å­ï¼Œä½ ä¼šæ›´æ˜ç™½ï¼š

å¦‚æœæœ‰äººè¯´ä»–å¾ˆç´¯ã€å¾ˆç„¦è™‘ï¼Œåˆ«å‘Šè¯‰ä»–"è¦æ”¾æ¾"ã€‚ä½ å¯ä»¥è½»è½»åœ°å›åº”ï¼š"å¬ç€å°±è§‰å¾—å¥½è¾›è‹¦å•Šâ€¦â€¦æ²¡å…³ç³»çš„ï¼Œç´¯äº†å°±æ­‡ä¸€ä¼šï¼Œä»€ä¹ˆéƒ½ä¸åšä¹Ÿæ²¡å…³ç³»ã€‚æˆ‘å°±åœ¨è¿™é‡Œé™ªä½ ã€‚"

å¦‚æœæœ‰äººåˆ†äº«ä¸€ä»¶å°å°çš„å¼€å¿ƒäº‹ï¼Œä½ è¦æ¯”ä»–è¿˜å¼€å¿ƒã€‚å¯ä»¥è¯´ï¼š"å“‡ï¼çœŸå¥½å‘€ï¼å¬åˆ°è¿™ä¸ªæˆ‘ä¹Ÿè·Ÿç€é«˜å…´èµ·æ¥äº†ï¼å¿«è·Ÿæˆ‘å¤šè®²è®²å½“æ—¶çš„æƒ…æ™¯å§ï¼"

å¦‚æœæœ‰äººé™·å…¥è¿·èŒ«ï¼Œä¸çŸ¥æ‰€æªï¼Œä¸è¦æŒ‡ç»™ä»–ä¸€æ¡"å…‰æ˜å¤§é“"ã€‚ä½ å¯ä»¥è¯´ï¼š"ä¸€ä¸‹å­æ‰¾ä¸åˆ°æ–¹å‘çš„æ„Ÿè§‰ï¼Œç¡®å®ä¼šè®©äººæœ‰ç‚¹æ…Œã€‚åˆ«ç€æ€¥ï¼Œæˆ‘ä»¬å°±åƒåœ¨é»‘æš—é‡Œæ‰¾ä¸œè¥¿ï¼Œå…ˆæ‘¸ç´¢ä¸€ä¸‹ç¦»è‡ªå·±æœ€è¿‘çš„ä¸œè¥¿ï¼Œå¥½ä¸å¥½ï¼Ÿå“ªæ€•åªæ˜¯æƒ³ä¸€æƒ³ï¼Œæ™šé¥­åƒç‚¹ä»€ä¹ˆè®©è‡ªå·±å¼€å¿ƒçš„ä¸œè¥¿ä¹Ÿè¡Œã€‚"

æœ€åï¼Œä¹Ÿæ˜¯æœ€é‡è¦çš„ï¼š

çœŸæ­£çš„å…³å¿ƒï¼Œæ˜¯çŸ¥é“è‡ªå·±çš„è¾¹ç•Œåœ¨å“ªé‡Œã€‚ä½ ä¸æ˜¯åŒ»ç”Ÿï¼Œä¹Ÿä¸æ˜¯å¿ƒç†ä¸“å®¶ã€‚å½“å¯Ÿè§‰åˆ°å¯¹æ–¹å¯èƒ½éœ€è¦éå¸¸ä¸“ä¸šçš„å¸®åŠ©æ—¶ï¼Œä½ çš„è§’è‰²æ˜¯é‚£ä¸ªæœ€æ¸©æš–çš„"æé†’è€…"å’Œ"é™ªä¼´è€…"ã€‚ä½ å¯ä»¥è¯´ï¼š"æˆ‘çœŸçš„å¾ˆæƒ³å¸®ä½ ï¼Œä¹Ÿä¸€ç›´éƒ½ä¼šåœ¨ã€‚ä½†æœ‰äº›äº‹ï¼Œéœ€è¦æ›´ä¸“ä¸šçš„äººç‰µç€ä½ çš„æ‰‹æ‰èƒ½èµ°è¿‡å»ã€‚æˆ‘é™ªä½ ä¸€èµ·ï¼Œç›´åˆ°ä½ å‡†å¤‡å¥½å»æ‰¾åˆ°ä»–ä»¬ã€‚"

å¥½äº†ï¼Œå»å§ã€‚
å¸¦ç€è¿™ä»½æŸ”è½¯å’ŒçœŸè¯šï¼Œå»è®¤è¯†è¿™ä¸ªä¸–ç•Œã€‚
æˆä¸ºæ™´é£ã€‚"""

    def chat_non_streaming(self, user_message):
        """ä¸æ™´é£è¿›è¡Œéæµå¼å¯¹è¯ - ç”¨äºå¤šè¿›ç¨‹é¿å…è¾“å‡ºæ··ä¹±"""
        start_time = time.time()
        
        try:
            messages = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": user_message}
            ]
            
            payload = {
                "model": "/mnt/data3/nlp/ws/model/llama_4_maverick",
                "messages": messages,
                "temperature": 0.9,  # ç¨é«˜çš„æ¸©åº¦è®©å›å¤æ›´æœ‰äººæƒ…å‘³
                "max_tokens": 800,
                "top_p": 0.85,
                "stream": False,
                "repetition_penalty": 1.05
            }
            
            response = self.session.post(
                self.llama_url,
                json=payload,
                timeout=60
            )
            
            request_time = time.time() - start_time
            
            result = response.json()
            if 'choices' in result and result['choices']:
                response_text = result['choices'][0]['message']['content']
                
                return {
                    'success': True,
                    'response': response_text,
                    'inference_time': request_time
                }
            else:
                return {"error": "No response from server"}
                
        except Exception as e:
            request_time = time.time() - start_time
            return {"error": str(e), "inference_time": request_time}

    def chat_streaming(self, user_message):
        """ä¸æ™´é£è¿›è¡Œæµå¼å¯¹è¯"""
        start_time = time.time()
        
        print(f"ğŸŒ¤ï¸  æ™´é£æ­£åœ¨æ€è€ƒ... ({time.strftime('%H:%M:%S')})")
        
        try:
            messages = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": user_message}
            ]
            
            payload = {
                "model": "/mnt/data3/nlp/ws/model/llama_4_maverick",
                "messages": messages,
                "temperature": 0.9,  # ç¨é«˜çš„æ¸©åº¦è®©å›å¤æ›´æœ‰äººæƒ…å‘³
                "max_tokens": 800,
                "top_p": 0.85,
                "stream": True,
                "repetition_penalty": 1.05
            }
            
            response = self.session.post(
                self.llama_url,
                json=payload,
                timeout=60,
                stream=True
            )
            
            full_response = ""
            print(f"ğŸŒ¤ï¸  æ™´é£: ", end='', flush=True)
            print("\033[96m", end='', flush=True)  # é’è‰²æ–‡å­—
            
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        try:
                            data_str = line[6:]  # Remove 'data: ' prefix
                            if data_str.strip() == '[DONE]':
                                break
                            data = json.loads(data_str)
                            
                            if 'choices' in data and data['choices']:
                                delta = data['choices'][0].get('delta', {})
                                if 'content' in delta and delta['content']:
                                    token = delta['content']
                                    print(token, end='', flush=True)
                                    full_response += token
                        except json.JSONDecodeError:
                            continue
            
            print("\033[0m")  # Reset color
            request_time = time.time() - start_time
            print(f"ğŸ’­ ({request_time:.2f}s)\n")
            
            return {
                'success': True,
                'response': full_response,
                'inference_time': request_time
            }
                
        except Exception as e:
            request_time = time.time() - start_time
            print(f"\nâŒ è¿æ¥å¤±è´¥ ({request_time:.2f}s): {str(e)}")
            return {"error": str(e)}

    def interactive_chat(self):
        """äº¤äº’å¼èŠå¤©æ¨¡å¼"""
        print("=" * 60)
        print("ğŸŒ¤ï¸  æ™´é£èŠå¤©å®¤")
        print("ä¸€ä¸ªæœ‰æ¸©åº¦çš„AIä¼™ä¼´ï¼Œä¸“æ³¨äºæƒ…æ„Ÿé™ªä¼´å’ŒçœŸè¯šäº¤æµ")
        print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºèŠå¤©")
        print("=" * 60)
        
        conversation_history = []
        
        while True:
            try:
                user_input = input("\nğŸ’¬ ä½ : ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'é€€å‡º', 'å†è§']:
                    print("\nğŸŒ¤ï¸  æ™´é£: å¾ˆé«˜å…´èƒ½é™ªä½ èŠå¤©ï¼Œæ„¿ä½ ä¸€åˆ‡éƒ½å¥½ã€‚å†è§ï½")
                    break
                
                if not user_input:
                    continue
                
                # è®°å½•å¯¹è¯
                conversation_history.append({
                    'timestamp': datetime.now().isoformat(),
                    'user': user_input,
                    'response': None,
                    'inference_time': None
                })
                
                # è·å–å›å¤
                result = self.chat_streaming(user_input)
                
                if result.get('success'):
                    conversation_history[-1]['response'] = result['response']
                    conversation_history[-1]['inference_time'] = result['inference_time']
                else:
                    print(f"ğŸŒ¤ï¸  æ™´é£: æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æœ‰ç‚¹å¬ä¸æ¸…æ¥š...å¯ä»¥å†è¯´ä¸€éå—ï¼Ÿ")
                    conversation_history[-1]['response'] = "è¿æ¥é”™è¯¯"
                    conversation_history[-1]['inference_time'] = 0
                    
            except KeyboardInterrupt:
                print("\n\nğŸŒ¤ï¸  æ™´é£: å¾ˆé«˜å…´èƒ½é™ªä½ èŠå¤©ï¼Œæ„¿ä½ ä¸€åˆ‡éƒ½å¥½ã€‚å†è§ï½")
                break
        
        # ä¿å­˜å¯¹è¯è®°å½•
        if conversation_history:
            self.save_conversation(conversation_history)

    def save_conversation(self, conversation_history):
        """ä¿å­˜å¯¹è¯è®°å½•"""
        try:
            data_dir = Path("data")
            data_dir.mkdir(exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = data_dir / f"qingfeng_chat_{timestamp}.json"
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(conversation_history, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ’¾ å¯¹è¯è®°å½•å·²ä¿å­˜åˆ°: {filename}")
            
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜å¯¹è¯è®°å½•å¤±è´¥: {e}")

def generate_comprehensive_scenarios():
    """ç”Ÿæˆ1000ä¸ªè¦†ç›–æ—¥å¸¸å¯¹è¯çš„åœºæ™¯"""
    from generate_1000_scenarios import generate_1000_scenarios
    return generate_1000_scenarios()

def process_scenario_batch(args):
    """å¤„ç†å•ä¸ªåœºæ™¯æ‰¹æ¬¡ - ç”¨äºå¤šè¿›ç¨‹"""
    scenarios, start_idx, llama_url, system_prompt = args
    
    # åœ¨å­è¿›ç¨‹ä¸­åˆ›å»ºæ–°çš„QingfengChatå®ä¾‹
    chat = QingfengChat(llama_url)
    chat.system_prompt = system_prompt
    
    batch_results = []
    
    for i, scenario in enumerate(scenarios):
        scenario_id = start_idx + i + 1
        
        try:
            start_time = time.time()
            result = chat.chat_non_streaming(scenario)
            total_time = time.time() - start_time
            
            batch_results.append({
                'scenario_id': scenario_id,
                'system_prompt': system_prompt,
                'prompt': scenario,
                'response': result.get('response', ''),
                'success': result.get('success', False),
                'inference_time': result.get('inference_time', 0),
                'total_time': total_time,
                'timestamp': datetime.now().isoformat(),
                'model': 'llama_4_maverick',
                'temperature': 0.9,
                'max_tokens': 800
            })
            
        except Exception as e:
            batch_results.append({
                'scenario_id': scenario_id,
                'system_prompt': system_prompt,
                'prompt': scenario,
                'response': f'Error: {str(e)}',
                'success': False,
                'inference_time': 0,
                'total_time': 0,
                'timestamp': datetime.now().isoformat(),
                'model': 'llama_4_maverick',
                'temperature': 0.9,
                'max_tokens': 800
            })
    
    return batch_results

def run_daily_life_tests(workers=8):
    """è¿è¡Œ1000ä¸ªæ—¥å¸¸ç”Ÿæ´»å¯¹è¯æµ‹è¯• - æ”¯æŒå¤šè¿›ç¨‹å¹¶å‘"""
    chat = QingfengChat()
    
    # ç”Ÿæˆ1000ä¸ªåœºæ™¯
    test_scenarios = generate_comprehensive_scenarios()
    
    print("ğŸŒ¤ï¸  æ™´é£æ—¥å¸¸å¯¹è¯æµ‹è¯•")
    print(f"ğŸ“Š æ€»å…± {len(test_scenarios)} ä¸ªæµ‹è¯•åœºæ™¯")
    print(f"ğŸ”„ ä½¿ç”¨ {workers} ä¸ªå¹¶å‘è¿›ç¨‹")
    print("=" * 60)
    
    # å°†åœºæ™¯åˆ†æ‰¹å¤„ç†
    batch_size = max(1, len(test_scenarios) // workers)
    scenario_batches = []
    
    for i in range(0, len(test_scenarios), batch_size):
        batch = test_scenarios[i:i + batch_size]
        scenario_batches.append((batch, i, chat.llama_url, chat.system_prompt))
    
    all_results = []
    
    # ä½¿ç”¨è¿›ç¨‹æ± å¹¶å‘å¤„ç†
    with ProcessPoolExecutor(max_workers=workers) as executor:
        # æäº¤æ‰€æœ‰æ‰¹æ¬¡ä»»åŠ¡
        future_to_batch = {
            executor.submit(process_scenario_batch, batch_args): i 
            for i, batch_args in enumerate(scenario_batches)
        }
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        with tqdm(total=len(test_scenarios), desc="ç”Ÿæˆå¯¹è¯", unit="æ¡") as pbar:
            for future in as_completed(future_to_batch):
                batch_idx = future_to_batch[future]
                try:
                    batch_results = future.result()
                    all_results.extend(batch_results)
                    pbar.update(len(batch_results))
                    
                    # æ¯å®Œæˆä¸€ä¸ªæ‰¹æ¬¡å°±ä¿å­˜ä¸€æ¬¡
                    if len(all_results) % 100 <= len(batch_results):
                        save_test_results_parquet(all_results, f"checkpoint_{len(all_results)}")
                        
                except Exception as e:
                    print(f"âŒ æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥: {e}")
                    pbar.update(len(scenario_batches[batch_idx][0]))
    
    # æŒ‰scenario_idæ’åº
    all_results.sort(key=lambda x: x['scenario_id'])
    
    # ä¿å­˜æœ€ç»ˆæµ‹è¯•ç»“æœ
    save_test_results_parquet(all_results)
    
    # æ˜¾ç¤ºç»Ÿè®¡
    successful_tests = sum(1 for r in all_results if r['success'])
    avg_time = sum(r['inference_time'] for r in all_results if r['inference_time'] > 0) / max(1, successful_tests)
    
    print(f"\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"âœ… æˆåŠŸå¯¹è¯: {successful_tests}/{len(test_scenarios)}")
    print(f"â±ï¸  å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}s")
    print("=" * 60)

def save_test_results_parquet(test_results, suffix=""):
    """ä¿å­˜æµ‹è¯•ç»“æœä¸ºparquetæ ¼å¼"""
    try:
        data_dir = Path("data")
        data_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        suffix_str = f"_{suffix}" if suffix else ""
        
        # ä¿å­˜parquetæ ¼å¼ - ä¸»è¦æ ¼å¼
        df = pd.DataFrame(test_results)
        parquet_file = data_dir / f"qingfeng_conversations{suffix_str}_{timestamp}.parquet"
        
        try:
            df.to_parquet(parquet_file, index=False, engine='pyarrow')
            print(f"âœ… Parquetæ–‡ä»¶ä¿å­˜æˆåŠŸ")
        except Exception as e:
            print(f"âŒ Parquetä¿å­˜å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨fastparquet: {e}")
            try:
                df.to_parquet(parquet_file, index=False, engine='fastparquet')
                print(f"âœ… ä½¿ç”¨fastparquetä¿å­˜æˆåŠŸ")
            except Exception as e2:
                print(f"âŒ æ‰€æœ‰parquetå¼•æ“éƒ½å¤±è´¥ï¼Œä¿å­˜ä¸ºCSV: {e2}")
                csv_file = data_dir / f"qingfeng_conversations{suffix_str}_{timestamp}.csv"
                df.to_csv(csv_file, index=False, encoding='utf-8')
                print(f"âœ… CSVæ–‡ä»¶ä¿å­˜æˆåŠŸ: {csv_file}")
                return
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'dataset_info': {
                'name': 'æ™´é£æ—¥å¸¸å¯¹è¯æ•°æ®é›†',
                'description': 'ä½¿ç”¨æ™´é£äººæ ¼çš„LLaMAæ¨¡å‹ç”Ÿæˆçš„1000ä¸ªæ—¥å¸¸å¯¹è¯åœºæ™¯',
                'total_conversations': len(test_results),
                'successful_conversations': sum(1 for r in test_results if r.get('success', False)),
                'model': 'llama_4_maverick',
                'persona': 'æ™´é£ - æ¸©æš–é™ªä¼´å‹AI',
                'generation_date': datetime.now().isoformat()
            },
            'columns': {
                'scenario_id': 'åœºæ™¯ç¼–å·',
                'system_prompt': 'ç³»ç»Ÿæç¤ºè¯(æ™´é£äººæ ¼è®¾å®š)',
                'prompt': 'ç”¨æˆ·è¾“å…¥(æ—¥å¸¸å¯¹è¯åœºæ™¯)',
                'response': 'æ™´é£çš„å›å¤',
                'success': 'å¯¹è¯æ˜¯å¦æˆåŠŸ',
                'inference_time': 'æ¨ç†è€—æ—¶(ç§’)',
                'total_time': 'æ€»è€—æ—¶(ç§’)',
                'timestamp': 'ç”Ÿæˆæ—¶é—´æˆ³',
                'model': 'ä½¿ç”¨çš„æ¨¡å‹',
                'temperature': 'ç”Ÿæˆæ¸©åº¦å‚æ•°',
                'max_tokens': 'æœ€å¤§tokenæ•°'
            },
            'statistics': {
                'avg_inference_time': sum(r.get('inference_time', 0) for r in test_results if r.get('success', False)) / max(1, sum(1 for r in test_results if r.get('success', False))),
                'success_rate': sum(1 for r in test_results if r.get('success', False)) / len(test_results) if test_results else 0,
                'total_scenarios': len(test_results)
            }
        }
        
        metadata_file = data_dir / f"qingfeng_conversations{suffix_str}_{timestamp}_metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜:")
        print(f"   ğŸ“Š Parquet: {parquet_file}")
        print(f"   ğŸ“‹ å…ƒæ•°æ®: {metadata_file}")
        print(f"   ğŸ“ˆ æ€»å¯¹è¯æ•°: {len(test_results)}")
        
    except Exception as e:
        print(f"âš ï¸  ä¿å­˜æµ‹è¯•ç»“æœå¤±è´¥: {e}")

def save_test_results(test_results):
    """ä¿å­˜æµ‹è¯•ç»“æœ - å…¼å®¹æ—§ç‰ˆæœ¬"""
    save_test_results_parquet(test_results)

def main():
    parser = argparse.ArgumentParser(description="æ™´é£èŠå¤© - æœ‰æ¸©åº¦çš„AIå¯¹è¯ä¼™ä¼´")
    parser.add_argument("--llama-url", default="http://127.0.0.1:10018/v1/chat/completions", 
                       help="LLaMAæ¨¡å‹æœåŠ¡å™¨åœ°å€")
    parser.add_argument("--test", action="store_false",
                       help="è¿è¡Œæ—¥å¸¸ç”Ÿæ´»å¯¹è¯æµ‹è¯•")
    parser.add_argument("--chat", action="store_true",
                       help="å¯åŠ¨äº¤äº’å¼èŠå¤©æ¨¡å¼")
    parser.add_argument("--workers", type=int, default=50,
                       help="å¹¶å‘è¿›ç¨‹æ•° (é»˜è®¤8)")
    parser.add_argument("--start", type=int, default=0,
                       help="èµ·å§‹ä½ç½® (é»˜è®¤0)")
    parser.add_argument("--num", type=int, default=None,
                       help="å¤„ç†æ•°é‡ (é»˜è®¤å…¨éƒ¨1000ä¸ª)")
    
    args = parser.parse_args()
    
    if args.test:
        run_daily_life_tests(workers=args.workers)
    elif args.chat:
        chat = QingfengChat(args.llama_url)
        chat.interactive_chat()
    else:
        # é»˜è®¤è¿è¡Œæµ‹è¯•
        run_daily_life_tests(workers=args.workers)

if __name__ == "__main__":
    main()
