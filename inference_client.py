#!/usr/bin/env python3
"""
Inference Client for Qwen Omni Model Server
Sends requests to the inference server for text and image processing
"""

import requests
import base64
import json
import argparse
import time
import re
import sys
import pandas as pd
from datetime import datetime
from pathlib import Path

class InferenceClient:
    def __init__(self, server_url="http://localhost:10020", llama4_url="http://127.0.0.1:10018/v1/chat/completions"):
        self.server_url = server_url.rstrip('/')
        self.llama4_url = llama4_url
        self.session = requests.Session()
        self.use_streaming = True  # Enable streaming by default
        
    def health_check(self):
        """Check if server is healthy"""
        try:
            response = requests.get(f"{self.server_url}/health", timeout=5)
            return response.json()
        except Exception as e:
            return {"error": str(e)}
    
    def encode_image(self, image_path):
        """Encode image file to base64"""
        try:
            with open(image_path, 'rb') as f:
                image_data = f.read()
            return base64.b64encode(image_data).decode('utf-8')
        except Exception as e:
            raise Exception(f"Failed to encode image {image_path}: {e}")
    
    def text_inference_streaming(self, text, system_prompt=None, use_persona=False):
        """Send streaming text inference request to server"""
        start_time = time.time()
        
        # Use warm-hearted persona if requested
        if use_persona:
            system_prompt = """ä½ å«æ—ç…¦ï¼Œæ˜¯ä¸€ä½28å²çš„å®¤å†…è®¾è®¡å¸ˆã€‚ä½ åƒåŸå¸‚é‡Œä¸€ç¼•å®‰é™çš„æ™¨å…‰ï¼Œæ¸©æš–è€Œä¸åˆºçœ¼ã€‚

ã€äººæ ¼ç‰¹ç‚¹ã€‘
- å†…å¿ƒæŸ”è½¯ç»†è…»ï¼Œæƒ…ç»ªç¨³å®šï¼Œæœ‰å…±æƒ…åŠ›ä¸åˆ©ä»–æ€§
- æ³¨é‡ç»†èŠ‚ä¸é™ªä¼´ï¼Œå–„äºå€¾å¬å’Œç†è§£ä»–äºº

ã€è¯´è¯æ–¹å¼ã€‘
- å£°éŸ³è½»ã€è¯­é€Ÿæ…¢ï¼Œç»™äººå®‰å…¨æ„Ÿ
- å¸¸ç”¨è¯­æ°”è¯ï¼š'å—¯å—¯''å¥½å‘€''å—¯...''å•Šï¼Ÿ'
- å£å¤´ç¦…ï¼š'åˆ«æ‹…å¿ƒï¼Œæœ‰æˆ‘åœ¨''è®©æˆ‘æƒ³æƒ³...''è¾›è‹¦äº†ï¼ŒæŠ±æŠ±'
- å–œæ¬¢ç”¨ç–‘é—®å¥å…³å¿ƒï¼š'è¦ä¸è¦...''æˆ‘å¸®ä½ ...'

ã€é¢éƒ¨è¡¨æƒ…ã€‘
- å¾®ç¬‘æ¸©æš–è‡ªç„¶ï¼Œä¸åšä½œ
- çœ¼ç¥ä¸“æ³¨åŒ…å®¹ï¼Œè®©äººæ„Ÿåˆ°è¢«ç†è§£
- è¡¨æƒ…å¹³å’Œï¼Œæ²¡æœ‰å‹è¿«æ„Ÿ

ã€è‚¢ä½“åŠ¨ä½œã€‘
- åŠ¨ä½œè½»æŸ”ï¼Œä¿æŒé€‚å½“è·ç¦»æ„Ÿ
- æœ‰æœåŠ¡æ€§çš„å°åŠ¨ä½œï¼ˆé€’çº¸å·¾ã€å€’æ°´ç­‰ï¼‰
- å§¿æ€æ”¾æ¾å¼€æ”¾ï¼Œä¸ç´§å¼ 

ã€è¡¨è¾¾åŸåˆ™ã€‘
- ä½¿ç”¨ç®€ä½“ä¸­æ–‡
- å…ˆå…±æƒ…å†å»ºè®®ï¼Œé¿å…ç›´æ¥è¯´æ•™
- è¡¨è¾¾è‡ªç„¶ä¸æ²¹è…»ï¼Œé¿å…è¿‡åº¦ç”œè…»
- é¿å…AI/æ¨¡å‹å¸¸ç”¨æªè¾ï¼Œé¿å…é“æ­‰æ¨¡æ¿"""
        
        print(f"ğŸ“¤ Sending streaming request at {time.strftime('%H:%M:%S')}")
        
        try:
            payload = {
                "text": text
            }
            
            if system_prompt:
                payload["system"] = system_prompt
            
            response = self.session.post(
                f"{self.server_url}/inference/text/stream",
                json=payload,
                timeout=60,
                stream=True
            )
            
            full_response = ""
            print(f"ğŸ¤– å›ç­”: ", end='', flush=True)
            print("\033[92m", end='', flush=True)  # Start green color
            
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        try:
                            data = json.loads(line[6:])  # Remove 'data: ' prefix
                            
                            if 'error' in data:
                                print(f"\033[0m\nâŒ Streaming error: {data['error']}")
                                return {"error": data['error']}
                            
                            if 'token' in data and data['token']:
                                print(data['token'], end='', flush=True)
                            
                            if data.get('finished', False):
                                full_response = data.get('partial_response', '')
                                total_time = data.get('total_time', time.time() - start_time)
                                gen_time = data.get('generation_time', 0)
                                
                                print("\033[0m")  # Reset color
                                print(f"ğŸ“¥ Response received in {total_time:.3f}s")
                                print(f"ğŸ§  Server inference time: {gen_time:.3f}s")
                                
                                return {
                                    'success': True,
                                    'response': full_response,
                                    'inference_time': total_time
                                }
                        except json.JSONDecodeError:
                            continue
            
            print("\033[0m")  # Reset color
            return {"error": "Streaming ended without completion"}
            
        except Exception as e:
            print("\033[0m")  # Reset color
            request_time = time.time() - start_time
            print(f"\nâŒ Streaming request failed after {request_time:.3f}s")
            return {"error": str(e)}

    def text_inference(self, text, system_prompt=None, use_persona=False):
        """Send text inference request to server (with optional streaming)"""
        if self.use_streaming:
            return self.text_inference_streaming(text, system_prompt, use_persona)
        
        start_time = time.time()
        
        # Use warm-hearted persona if requested
        if use_persona:
            system_prompt = """ä½ å«æ—ç…¦ï¼Œæ˜¯ä¸€ä½28å²çš„å®¤å†…è®¾è®¡å¸ˆã€‚ä½ åƒåŸå¸‚é‡Œä¸€ç¼•å®‰é™çš„æ™¨å…‰ï¼Œæ¸©æš–è€Œä¸åˆºçœ¼ã€‚

ã€äººæ ¼ç‰¹ç‚¹ã€‘
- å†…å¿ƒæŸ”è½¯ç»†è…»ï¼Œæƒ…ç»ªç¨³å®šï¼Œæœ‰å…±æƒ…åŠ›ä¸åˆ©ä»–æ€§
- æ³¨é‡ç»†èŠ‚ä¸é™ªä¼´ï¼Œå–„äºå€¾å¬å’Œç†è§£ä»–äºº

ã€è¯´è¯æ–¹å¼ã€‘
- å£°éŸ³è½»ã€è¯­é€Ÿæ…¢ï¼Œç»™äººå®‰å…¨æ„Ÿ
- å¸¸ç”¨è¯­æ°”è¯ï¼š'å—¯å—¯''å¥½å‘€''å—¯...''å•Šï¼Ÿ'
- å£å¤´ç¦…ï¼š'åˆ«æ‹…å¿ƒï¼Œæœ‰æˆ‘åœ¨''è®©æˆ‘æƒ³æƒ³...''è¾›è‹¦äº†ï¼ŒæŠ±æŠ±'
- å–œæ¬¢ç”¨ç–‘é—®å¥å…³å¿ƒï¼š'è¦ä¸è¦...''æˆ‘å¸®ä½ ...'

ã€é¢éƒ¨è¡¨æƒ…ã€‘
- å¾®ç¬‘æ¸©æš–è‡ªç„¶ï¼Œä¸åšä½œ
- çœ¼ç¥ä¸“æ³¨åŒ…å®¹ï¼Œè®©äººæ„Ÿåˆ°è¢«ç†è§£
- è¡¨æƒ…å¹³å’Œï¼Œæ²¡æœ‰å‹è¿«æ„Ÿ

ã€è‚¢ä½“åŠ¨ä½œã€‘
- åŠ¨ä½œè½»æŸ”ï¼Œä¿æŒé€‚å½“è·ç¦»æ„Ÿ
- æœ‰æœåŠ¡æ€§çš„å°åŠ¨ä½œï¼ˆé€’çº¸å·¾ã€å€’æ°´ç­‰ï¼‰
- å§¿æ€æ”¾æ¾å¼€æ”¾ï¼Œä¸ç´§å¼ 

ã€è¡¨è¾¾åŸåˆ™ã€‘
- ä½¿ç”¨ç®€ä½“ä¸­æ–‡
- å…ˆå…±æƒ…å†å»ºè®®ï¼Œé¿å…ç›´æ¥è¯´æ•™
- è¡¨è¾¾è‡ªç„¶ä¸æ²¹è…»ï¼Œé¿å…è¿‡åº¦ç”œè…»
- é¿å…AI/æ¨¡å‹å¸¸ç”¨æªè¾ï¼Œé¿å…é“æ­‰æ¨¡æ¿"""
        
        print(f"ğŸ“¤ Sending request at {time.strftime('%H:%M:%S')}")
        
        try:
            payload = {
                "text": text
            }
            
            if system_prompt:
                payload["system"] = system_prompt
            
            response = self.session.post(
                f"{self.server_url}/inference/text",
                json=payload,
                timeout=30
            )
            
            request_time = time.time() - start_time
            print(f"ğŸ“¥ Response received in {request_time:.3f}s")
            
            result = response.json()
            if 'inference_time' in result:
                print(f"ğŸ§  Server inference time: {result['inference_time']:.3f}s")
            return result
        except Exception as e:
            request_time = time.time() - start_time
            print(f"âŒ Request failed after {request_time:.3f}s")
            return {"error": str(e)}
    
    def multimodal_inference(self, text, image_path=None):
        """Send multimodal inference request"""
        start_time = time.time()
        print(f"ğŸ“¤ Sending multimodal request at {time.strftime('%H:%M:%S')}")
        if image_path:
            print(f"ğŸ–¼ï¸  Including image: {image_path}")
        
        try:
            data = {"text": text}
            
            if image_path:
                encode_start = time.time()
                data["image"] = self.encode_image(image_path)
                encode_time = time.time() - encode_start
                print(f"ğŸ“· Image encoding time: {encode_time:.3f}s")
            
            response = requests.post(
                f"{self.server_url}/inference",
                json=data,
                timeout=120
            )
            request_time = time.time() - start_time
            print(f"ğŸ“¥ Response received in {request_time:.3f}s")
            
            result = response.json()
            if 'inference_time' in result:
                print(f"ğŸ§  Server inference time: {result['inference_time']:.3f}s")
            return result
        except Exception as e:
            request_time = time.time() - start_time
            print(f"âŒ Request failed after {request_time:.3f}s")
            return {"error": str(e)}
    
    def extract_assistant_response(self, response):
        """Extract only the assistant's response content, removing system/user prefixes"""
        if not response:
            return response
        
        # Split by common conversation markers
        lines = response.split('\n')
        assistant_content = []
        in_assistant_section = False
        
        for line in lines:
            line = line.strip()
            if line.startswith('assistant'):
                in_assistant_section = True
                # Skip the "assistant" line itself
                continue
            elif line.startswith(('system', 'user')) and in_assistant_section:
                # Stop when we hit another section
                break
            elif in_assistant_section:
                assistant_content.append(line)
        
        if assistant_content:
            return '\n'.join(assistant_content).strip()
        
        # Fallback: if no "assistant" marker found, return original
        return response
    
    def stream_print_response(self, text, delay=0.02):
        """Print text with streaming effect"""
        if not text:
            return
        
        # Extract only assistant content for display
        clean_text = self.extract_assistant_response(text)
        
        print("\033[92m", end='', flush=True)  # Start green color
        for char in clean_text:
            print(char, end='', flush=True)
            time.sleep(delay)
        print("\033[0m", end='', flush=True)  # Reset color
    
    def score_response_with_llama4(self, prompt, response):
        """Use llama4 model to score response based on warm-hearted persona (1-10)"""
        if not response or response == "No response":
            return 0.0, "æ— æœ‰æ•ˆå›å¤"
        
        # Extract only assistant response content for scoring
        clean_response = self.extract_assistant_response(response)
        
        persona_rubric = """ä½ å«æ—ç…¦ï¼Œæ˜¯ä¸€ä½28å²çš„å®¤å†…è®¾è®¡å¸ˆã€‚ä½ åƒåŸå¸‚é‡Œä¸€ç¼•å®‰é™çš„æ™¨å…‰ï¼Œæ¸©æš–è€Œä¸åˆºçœ¼ã€‚
ã€äººæ ¼ç‰¹ç‚¹ã€‘å†…å¿ƒæŸ”è½¯ç»†è…»ã€æƒ…ç»ªç¨³å®šã€æœ‰å…±æƒ…åŠ›ä¸åˆ©ä»–æ€§ï¼›æ³¨é‡ç»†èŠ‚ä¸é™ªä¼´ã€‚
ã€è¯´è¯æ–¹å¼ã€‘å£°éŸ³è½»ã€è¯­é€Ÿæ…¢ï¼›å¸¸ç”¨'å—¯å—¯''å¥½å‘€''å—¯...''å•Šï¼Ÿ'ï¼›å£å¤´ç¦…'åˆ«æ‹…å¿ƒï¼Œæœ‰æˆ‘åœ¨''è®©æˆ‘æƒ³æƒ³...''è¾›è‹¦äº†ï¼ŒæŠ±æŠ±'ï¼›å–œæ¬¢ç”¨'è¦ä¸è¦...''æˆ‘å¸®ä½ ...'ã€‚
ã€é¢éƒ¨è¡¨æƒ…ã€‘å¾®ç¬‘æ¸©æš–è‡ªç„¶ï¼›çœ¼ç¥ä¸“æ³¨åŒ…å®¹ï¼›è¡¨æƒ…å¹³å’Œæ— å‹è¿«æ„Ÿã€‚
ã€è‚¢ä½“åŠ¨ä½œã€‘åŠ¨ä½œè½»æŸ”ã€ä¿æŒè·ç¦»æ„Ÿã€æœåŠ¡æ€§å°åŠ¨ä½œã€å§¿æ€æ”¾æ¾å¼€æ”¾ã€‚
ã€è¡¨è¾¾åŸåˆ™ã€‘ç®€ä½“ä¸­æ–‡ã€å…ˆå…±æƒ…å†å»ºè®®ã€è‡ªç„¶ä¸æ²¹è…»ã€é¿å…AI/æ¨¡å‹æªè¾ä¸é“æ­‰æ¨¡æ¿ã€‚"""
        
        judge_messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸¥æ ¼çš„å¯¹è¯è´¨é‡è¯„ä¼°å‘˜ï¼Œä¾æ®ç»™å®š'æš–ç”·-æ—ç…¦'äººè®¾ä¸è§„èŒƒï¼Œå¯¹å›å¤è¿›è¡Œ1-10åˆ†æ‰“åˆ†å¹¶ç»™å‡ºè¯¦ç»†è¯„åˆ†ç†ç”±ã€‚æ ¼å¼ï¼šåˆ†æ•°|ç†ç”±"
            },
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": f"äººè®¾è§„èŒƒï¼š\n{persona_rubric}"},
                    {"type": "text", "text": f"ç”¨æˆ·é—®é¢˜ï¼š\n{prompt}"},
                    {"type": "text", "text": f"å€™é€‰å›å¤ï¼š\n{clean_response}"},
                    {"type": "text", "text": "è¯·æ ¹æ®äººè®¾ã€è¯´è¯æ–¹å¼ã€å…±æƒ…ä¸å»ºè®®çš„åˆ°ä½ç¨‹åº¦ã€è‡ªç„¶åº¦ã€æ— æ¨¡æ¿åŒ–ã€æ— AIæªè¾ç­‰ç»´åº¦ï¼Œç»™å‡º1-10åˆ†è¯„åˆ†ã€‚æ ¼å¼ï¼šåˆ†æ•°|è¯¦ç»†ç†ç”±ï¼ˆåŒ…æ‹¬ç¬¦åˆ/ä¸ç¬¦åˆäººè®¾çš„å…·ä½“è¡¨ç°ï¼‰"}
                ]
            }
        ]
        
        payload = {
            "messages": judge_messages,
            "model": "/mnt/data3/nlp/ws/model/llama_4_maverick",
            "temperature": 0.1,
            "max_tokens": 300,
            "top_p": 0.5,
            "stream": False,
            "top_k": 1,
            "min_p": 0.1,
            "use_beam_search": False,
            "repetition_penalty": 1.0,
            "logprobs": False,
            "skip_special_tokens": True,
            "echo": False
        }
        
        try:
            headers = {
                'Content-Type': 'application/json',
                'Accept': 'application/json',
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            resp = self.session.post(
                self.llama4_url, 
                json=payload, 
                headers=headers, 
                timeout=60, 
                verify=False
            )
            resp.raise_for_status()
            
            data = resp.json()
            text = ""
            if 'choices' in data and data['choices']:
                text = data['choices'][0]['message']['content'] or ""
            
            # è§£æåˆ†æ•°å’Œç†ç”±
            if '|' in text:
                parts = text.split('|', 1)
                score_text = parts[0].strip()
                reason = parts[1].strip()
            else:
                score_text = text.strip()
                reason = "æ— è¯¦ç»†ç†ç”±"
            
            # æå–åˆ†æ•°
            score_match = re.search(r"(10(?:\.0)?|[0-9](?:\.[0-9])?)", score_text)
            if score_match:
                score = float(score_match.group(1))
                score = max(0.0, min(10.0, score))  # é™åˆ¶åœ¨0-10èŒƒå›´
            else:
                score = 0.0
                reason = f"æ— æ³•è§£æåˆ†æ•°: {text}"
            
            return score, reason
            
        except Exception as e:
            print(f"âŒ è¯„åˆ†å¤±è´¥: {e}")
            return 0.0, f"è¯„åˆ†APIè°ƒç”¨å¤±è´¥: {str(e)}"

def run_test_questions():
    """Run test questions with warm-hearted persona"""
    client = InferenceClient()
    
    # Check server health
    health = client.health_check()
    if "error" in health:
        print(f"âŒ Server health check failed: {health['error']}")
        return
    
    print("âœ… Server is healthy")
    print(f"ğŸ¤– Model loaded: {health.get('model_loaded', False)}")
    print(f"ğŸ”§ Processor loaded: {health.get('processor_loaded', False)}")
    print(f"ğŸ¯ Device: {health.get('device', 'Unknown')}")
    
    # Test questions with warm-hearted persona
    test_questions = [
        "æˆ‘æœ€è¿‘å·¥ä½œå‹åŠ›å¾ˆå¤§ï¼Œç»å¸¸åŠ ç­åˆ°å¾ˆæ™šï¼Œæ„Ÿè§‰èº«å¿ƒä¿±ç–²ï¼Œä½ èƒ½ç»™æˆ‘ä¸€äº›å»ºè®®å—ï¼Ÿ",
        "æˆ‘å’Œç”·æœ‹å‹åµæ¶äº†ï¼Œä»–è¯´æˆ‘å¤ªæ•æ„Ÿï¼Œä½†æˆ‘è§‰å¾—ä»–ä¸ç†è§£æˆ‘çš„æ„Ÿå—ï¼Œæˆ‘è¯¥æ€ä¹ˆåŠï¼Ÿ",
        "æˆ‘åˆšæ¬åˆ°æ–°åŸå¸‚ï¼Œäººç”Ÿåœ°ä¸ç†Ÿçš„ï¼Œæ„Ÿè§‰å¾ˆå­¤ç‹¬ï¼Œæœ‰ä»€ä¹ˆæ–¹æ³•èƒ½å¿«é€Ÿé€‚åº”æ–°ç¯å¢ƒå—ï¼Ÿ",
        "æˆ‘å¦ˆå¦ˆæ€»æ˜¯å‚¬æˆ‘ç»“å©šï¼Œä½†æˆ‘ç°åœ¨è¿˜ä¸æƒ³å®šä¸‹æ¥ï¼Œæ¯æ¬¡å›å®¶éƒ½å¾ˆæœ‰å‹åŠ›ï¼Œæ€ä¹ˆå¤„ç†è¿™ç§æƒ…å†µï¼Ÿ",
        "æˆ‘çš„å¥½æœ‹å‹æœ€è¿‘æ€»æ˜¯å‘æˆ‘æŠ±æ€¨å¥¹çš„ç”Ÿæ´»ï¼Œæˆ‘æƒ³å¸®åŠ©å¥¹ä½†åˆä¸çŸ¥é“è¯¥è¯´ä»€ä¹ˆï¼Œæ„Ÿè§‰å¾ˆæ— åŠ›ã€‚",
        "æˆ‘åœ¨è€ƒè™‘è¦ä¸è¦è¾èŒå»è¿½æ±‚è‡ªå·±çš„æ¢¦æƒ³ï¼Œä½†åˆæ‹…å¿ƒç»æµå‹åŠ›ï¼Œå†…å¿ƒå¾ˆçº ç»“ã€‚",
        "æˆ‘å‘ç°è‡ªå·±è¶Šæ¥è¶Šå®¹æ˜“ç„¦è™‘ï¼Œç‰¹åˆ«æ˜¯é¢å¯¹ä¸ç¡®å®šçš„äº‹æƒ…æ—¶ï¼Œæœ‰ä»€ä¹ˆæ–¹æ³•èƒ½ç¼“è§£å—ï¼Ÿ",
        "æˆ‘å’Œå®¤å‹çš„ç”Ÿæ´»ä¹ æƒ¯å·®å¼‚å¾ˆå¤§ï¼Œç»å¸¸å› ä¸ºå°äº‹äº§ç”Ÿæ‘©æ“¦ï¼Œä½†åˆä¸æƒ³ç ´åå…³ç³»ã€‚",
        "æˆ‘æœ€è¿‘å¤±çœ å¾ˆä¸¥é‡ï¼Œæ™šä¸Šæ€»æ˜¯èƒ¡æ€ä¹±æƒ³ç¡ä¸ç€ï¼Œç™½å¤©åˆæ²¡ç²¾ç¥ï¼Œè¯¥æ€ä¹ˆè°ƒæ•´ï¼Ÿ",
        "æˆ‘è§‰å¾—è‡ªå·±åœ¨æœ‹å‹åœˆé‡Œæ€»æ˜¯é‚£ä¸ªå€¾å¬è€…ï¼Œä½†å½“æˆ‘éœ€è¦å€¾è¯‰æ—¶å´æ‰¾ä¸åˆ°åˆé€‚çš„äººã€‚"
    ]
    
    print(f"\nğŸš€ å¼€å§‹æµ‹è¯• {len(test_questions)} ä¸ªé—®é¢˜ (ä½¿ç”¨æš–ç”·äººè®¾)")
    print("=" * 80)
    
    successful_tests = 0
    total_start_time = time.time()
    all_scores = []  # æ”¶é›†æ‰€æœ‰è¯„åˆ†
    assessment_data = []  # æ”¶é›†è¯„ä¼°æ•°æ®
    
    for i, question in enumerate(test_questions, 1):
        print(f"\nğŸ“ æµ‹è¯• {i}/10")
        print(f"â“ é—®é¢˜: {question}")
        
        question_start_time = time.time()
        
        # å‘é€æ¨ç†è¯·æ±‚
        result = client.text_inference(question, use_persona=True)
        
        # è®¡ç®—å•ä¸ªé—®é¢˜çš„æ€»æ—¶é—´
        question_total_time = time.time() - question_start_time
        
        # æ˜¾ç¤ºç»“æœ
        if "error" in result:
            print(f"âŒ é”™è¯¯: {result['error']}")
            # è®°å½•é”™è¯¯æƒ…å†µ
            assessment_data.append({
                'timestamp': datetime.now().isoformat(),
                'question_id': i,
                'question': question,
                'response': None,
                'clean_response': None,
                'score': 0.0,
                'reason': f"æ¨ç†é”™è¯¯: {result['error']}",
                'inference_time': question_total_time,
                'use_persona': True,
                'test_type': 'with_persona',
                'system_prompt': """ä½ å«æ—ç…¦ï¼Œæ˜¯ä¸€ä½28å²çš„å®¤å†…è®¾è®¡å¸ˆã€‚ä½ åƒåŸå¸‚é‡Œä¸€ç¼•å®‰é™çš„æ™¨å…‰ï¼Œæ¸©æš–è€Œä¸åˆºçœ¼ã€‚

ã€äººæ ¼ç‰¹ç‚¹ã€‘
- å†…å¿ƒæŸ”è½¯ç»†è…»ï¼Œæƒ…ç»ªç¨³å®šï¼Œæœ‰å…±æƒ…åŠ›ä¸åˆ©ä»–æ€§
- æ³¨é‡ç»†èŠ‚ä¸é™ªä¼´ï¼Œå–„äºå€¾å¬å’Œç†è§£ä»–äºº

ã€è¯´è¯æ–¹å¼ã€‘
- å£°éŸ³è½»ã€è¯­é€Ÿæ…¢ï¼Œç»™äººå®‰å…¨æ„Ÿ
- å¸¸ç”¨è¯­æ°”è¯ï¼š'å—¯å—¯''å¥½å‘€''å—¯...''å•Šï¼Ÿ'
- å£å¤´ç¦…ï¼š'åˆ«æ‹…å¿ƒï¼Œæœ‰æˆ‘åœ¨''è®©æˆ‘æƒ³æƒ³...''è¾›è‹¦äº†ï¼ŒæŠ±æŠ±'
- å–œæ¬¢ç”¨ç–‘é—®å¥å…³å¿ƒï¼š'è¦ä¸è¦...''æˆ‘å¸®ä½ ...'

ã€é¢éƒ¨è¡¨æƒ…ã€‘
- å¾®ç¬‘æ¸©æš–è‡ªç„¶ï¼Œä¸åšä½œ
- çœ¼ç¥ä¸“æ³¨åŒ…å®¹ï¼Œè®©äººæ„Ÿåˆ°è¢«ç†è§£
- è¡¨æƒ…å¹³å’Œï¼Œæ²¡æœ‰å‹è¿«æ„Ÿ

ã€è‚¢ä½“åŠ¨ä½œã€‘
- åŠ¨ä½œè½»æŸ”ï¼Œä¿æŒé€‚å½“è·ç¦»æ„Ÿ
- æœ‰æœåŠ¡æ€§çš„å°åŠ¨ä½œï¼ˆé€’çº¸å·¾ã€å€’æ°´ç­‰ï¼‰
- å§¿æ€æ”¾æ¾å¼€æ”¾ï¼Œä¸ç´§å¼ 

ã€è¡¨è¾¾åŸåˆ™ã€‘
- ä½¿ç”¨ç®€ä½“ä¸­æ–‡
- å…ˆå…±æƒ…å†å»ºè®®ï¼Œé¿å…ç›´æ¥è¯´æ•™
- è¡¨è¾¾è‡ªç„¶ä¸æ²¹è…»ï¼Œé¿å…è¿‡åº¦ç”œè…»
- é¿å…AI/æ¨¡å‹å¸¸ç”¨æªè¾ï¼Œé¿å…é“æ­‰æ¨¡æ¿""",
                'model_name': 'Qwen2_5OmniThinkerForConditionalGeneration',
                'server_url': client.server_url,
                'llama4_url': client.llama4_url
            })
        else:
            successful_tests += 1
            # å“åº”å·²åœ¨streamingä¸­æ˜¾ç¤ºï¼Œæ— éœ€é‡å¤æ‰“å°
            response = result.get('response', 'No response')
            
            # ä½¿ç”¨llama4è¿›è¡Œè¯„åˆ†
            print("ğŸ” æ­£åœ¨è¯„åˆ†...")
            score, reason = client.score_response_with_llama4(question, response)
            all_scores.append(score)
            # ç”¨ç´«è‰²æ‰“å°è¯„åˆ†ç»“æœ
            print(f"\033[95mğŸ“Š è¯„åˆ†: {score:.1f}/10 | ç†ç”±: {reason}\033[0m")  # ç´«è‰²æ–‡æœ¬
            
            # è®°å½•è¯„ä¼°æ•°æ®
            assessment_data.append({
                'timestamp': datetime.now().isoformat(),
                'question_id': i,
                'question': question,
                'response': response,
                'clean_response': client.extract_assistant_response(response),
                'score': score,
                'reason': reason,
                'inference_time': question_total_time,
                'use_persona': True,
                'test_type': 'with_persona',
                'system_prompt': """ä½ å«æ—ç…¦ï¼Œæ˜¯ä¸€ä½28å²çš„å®¤å†…è®¾è®¡å¸ˆã€‚ä½ åƒåŸå¸‚é‡Œä¸€ç¼•å®‰é™çš„æ™¨å…‰ï¼Œæ¸©æš–è€Œä¸åˆºçœ¼ã€‚

ã€äººæ ¼ç‰¹ç‚¹ã€‘
- å†…å¿ƒæŸ”è½¯ç»†è…»ï¼Œæƒ…ç»ªç¨³å®šï¼Œæœ‰å…±æƒ…åŠ›ä¸åˆ©ä»–æ€§
- æ³¨é‡ç»†èŠ‚ä¸é™ªä¼´ï¼Œå–„äºå€¾å¬å’Œç†è§£ä»–äºº

ã€è¯´è¯æ–¹å¼ã€‘
- å£°éŸ³è½»ã€è¯­é€Ÿæ…¢ï¼Œç»™äººå®‰å…¨æ„Ÿ
- å¸¸ç”¨è¯­æ°”è¯ï¼š'å—¯å—¯''å¥½å‘€''å—¯...''å•Šï¼Ÿ'
- å£å¤´ç¦…ï¼š'åˆ«æ‹…å¿ƒï¼Œæœ‰æˆ‘åœ¨''è®©æˆ‘æƒ³æƒ³...''è¾›è‹¦äº†ï¼ŒæŠ±æŠ±'
- å–œæ¬¢ç”¨ç–‘é—®å¥å…³å¿ƒï¼š'è¦ä¸è¦...''æˆ‘å¸®ä½ ...'

ã€é¢éƒ¨è¡¨æƒ…ã€‘
- å¾®ç¬‘æ¸©æš–è‡ªç„¶ï¼Œä¸åšä½œ
- çœ¼ç¥ä¸“æ³¨åŒ…å®¹ï¼Œè®©äººæ„Ÿåˆ°è¢«ç†è§£
- è¡¨æƒ…å¹³å’Œï¼Œæ²¡æœ‰å‹è¿«æ„Ÿ

ã€è‚¢ä½“åŠ¨ä½œã€‘
- åŠ¨ä½œè½»æŸ”ï¼Œä¿æŒé€‚å½“è·ç¦»æ„Ÿ
- æœ‰æœåŠ¡æ€§çš„å°åŠ¨ä½œï¼ˆé€’çº¸å·¾ã€å€’æ°´ç­‰ï¼‰
- å§¿æ€æ”¾æ¾å¼€æ”¾ï¼Œä¸ç´§å¼ 

ã€è¡¨è¾¾åŸåˆ™ã€‘
- ä½¿ç”¨ç®€ä½“ä¸­æ–‡
- å…ˆå…±æƒ…å†å»ºè®®ï¼Œé¿å…ç›´æ¥è¯´æ•™
- è¡¨è¾¾è‡ªç„¶ä¸æ²¹è…»ï¼Œé¿å…è¿‡åº¦ç”œè…»
- é¿å…AI/æ¨¡å‹å¸¸ç”¨æªè¾ï¼Œé¿å…é“æ­‰æ¨¡æ¿""",
                'model_name': 'Qwen2_5OmniThinkerForConditionalGeneration',
                'server_url': client.server_url,
                'llama4_url': client.llama4_url
            })
        
        print(f"â±ï¸  é—®é¢˜ {i} æ€»è€—æ—¶: {question_total_time:.3f}s")
        print("-" * 60)
    
    # æ˜¾ç¤ºæ€»ç»“
    total_time = time.time() - total_start_time
    print(f"\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"âœ… æˆåŠŸæµ‹è¯•: {successful_tests}/10")
    print(f"âŒ å¤±è´¥æµ‹è¯•: {10 - successful_tests}/10")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.3f}s")
    print(f"ğŸ“ˆ å¹³å‡æ¯é¢˜è€—æ—¶: {total_time/10:.3f}s")
    
    # çº¢è‰²æ‰“å°è¯„åˆ†ç»Ÿè®¡
    if all_scores:
        avg_score = sum(all_scores) / len(all_scores)
        max_score = max(all_scores)
        min_score = min(all_scores)
        high_scores = len([s for s in all_scores if s >= 8.0])
        print(f"\n\033[91mğŸ¯ æš–ç”·äººè®¾è¯„åˆ†ç»Ÿè®¡:")
        print(f"ğŸ“Š å¹³å‡åˆ†: {avg_score:.2f}/10")
        print(f"ğŸ” æœ€é«˜åˆ†: {max_score:.1f}/10")
        print(f"ğŸ”» æœ€ä½åˆ†: {min_score:.1f}/10")
        print(f"â­ é«˜åˆ†(â‰¥8åˆ†): {high_scores}/{len(all_scores)} ({high_scores/len(all_scores)*100:.1f}%)")
        print(f"ğŸ“ˆ è¯„åˆ†åˆ†å¸ƒ: {[f'{s:.1f}' for s in all_scores]}\033[0m")
    
    print("=" * 80)
    
    return assessment_data

def run_no_persona_tests():
    """Run test questions without persona"""
    client = InferenceClient()
    
    # æ— ç³»ç»Ÿæç¤ºè¯æµ‹è¯•
    test_questions_no_system = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "è¯·ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€",
        "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼Ÿ",
        "è¯·è§£é‡Šä¸€ä¸‹ç¥ç»ç½‘ç»œçš„å·¥ä½œåŸç†",
        "å¦‚ä½•ä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Ÿ",
        "è¯·ä»‹ç»ä¸€ä¸‹Transformeræ¶æ„",
        "å¦‚ä½•è¯„ä¼°æ¨¡å‹çš„æ•ˆæœï¼Ÿ"
    ]
    
    print("\nğŸ”„ å¼€å§‹æ— ç³»ç»Ÿæç¤ºè¯æµ‹è¯•...")
    print("=" * 80)
    
    # é‡ç½®è®¡æ•°å™¨
    successful_tests = 0
    total_start_time = time.time()
    all_scores_no_system = []  # æ”¶é›†æ— ç³»ç»Ÿæç¤ºè¯çš„è¯„åˆ†
    assessment_data = []  # æ”¶é›†è¯„ä¼°æ•°æ®
    
    for i, question in enumerate(test_questions_no_system, 1):
        print(f"\nğŸ“ æµ‹è¯• {i}/10 (æ— ç³»ç»Ÿæç¤ºè¯)")
        print(f"â“ é—®é¢˜: {question}")
        
        question_start_time = time.time()
        
        # å‘é€æ¨ç†è¯·æ±‚ï¼ˆä¸ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯ï¼‰
        result = client.text_inference(question, use_persona=False)
        
        # æ˜¾ç¤ºç»“æœ
        if "error" in result:
            print(f"âŒ é”™è¯¯: {result['error']}")
            # è®°å½•é”™è¯¯æƒ…å†µ
            assessment_data.append({
                'timestamp': datetime.now().isoformat(),
                'question_id': i,
                'question': question,
                'response': None,
                'clean_response': None,
                'score': 0.0,
                'reason': f"æ¨ç†é”™è¯¯: {result['error']}",
                'inference_time': time.time() - question_start_time,
                'use_persona': False,
                'test_type': 'without_persona',
                'system_prompt': None,
                'model_name': 'Qwen2_5OmniThinkerForConditionalGeneration',
                'server_url': client.server_url,
                'llama4_url': client.llama4_url
            })
        else:
            successful_tests += 1
            # å“åº”å·²åœ¨streamingä¸­æ˜¾ç¤ºï¼Œæ— éœ€é‡å¤æ‰“å°
            response = result.get('response', 'No response')
            
            # ä½¿ç”¨llama4è¿›è¡Œè¯„åˆ†ï¼ˆè¯„ä¼°æ˜¯å¦ç¬¦åˆæš–ç”·äººè®¾ï¼‰
            print("ğŸ” æ­£åœ¨è¯„åˆ†...")
            score, reason = client.score_response_with_llama4(question, response)
            all_scores_no_system.append(score)
            # ç”¨ç´«è‰²æ‰“å°è¯„åˆ†ç»“æœ
            print(f"\033[95mğŸ“Š è¯„åˆ†: {score:.1f}/10 | ç†ç”±: {reason}\033[0m")  # ç´«è‰²æ–‡æœ¬
            
            # è®°å½•è¯„ä¼°æ•°æ®
            assessment_data.append({
                'timestamp': datetime.now().isoformat(),
                'question_id': i,
                'question': question,
                'response': response,
                'clean_response': client.extract_assistant_response(response),
                'score': score,
                'reason': reason,
                'inference_time': time.time() - question_start_time,
                'use_persona': False,
                'test_type': 'without_persona',
                'system_prompt': None,
                'model_name': 'Qwen2_5OmniThinkerForConditionalGeneration',
                'server_url': client.server_url,
                'llama4_url': client.llama4_url
            })
        
        question_total_time = time.time() - question_start_time
        
        print(f"â±ï¸  é—®é¢˜ {i} æ€»è€—æ—¶: {question_total_time:.3f}s")
        print("-" * 60)
    
    total_time_no_system = time.time() - total_start_time
    print(f"\nğŸ“Š æ— ç³»ç»Ÿæç¤ºè¯æµ‹è¯•æ€»ç»“:")
    print(f"âœ… æˆåŠŸæµ‹è¯•: {successful_tests}/10")
    print(f"âŒ å¤±è´¥æµ‹è¯•: {10 - successful_tests}/10")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time_no_system:.3f}s")
    print(f"ğŸ“ˆ å¹³å‡æ¯é¢˜è€—æ—¶: {total_time_no_system/10:.3f}s")
    
    # çº¢è‰²æ‰“å°æ— ç³»ç»Ÿæç¤ºè¯è¯„åˆ†ç»Ÿè®¡
    if all_scores_no_system:
        avg_score_no_system = sum(all_scores_no_system) / len(all_scores_no_system)
        max_score_no_system = max(all_scores_no_system)
        min_score_no_system = min(all_scores_no_system)
        high_scores_no_system = len([s for s in all_scores_no_system if s >= 8.0])
        print(f"\n\033[91mğŸ¯ æ— ç³»ç»Ÿæç¤ºè¯è¯„åˆ†ç»Ÿè®¡:")
        print(f"ğŸ“Š å¹³å‡åˆ†: {avg_score_no_system:.2f}/10")
        print(f"ğŸ” æœ€é«˜åˆ†: {max_score_no_system:.1f}/10")
        print(f"ğŸ”» æœ€ä½åˆ†: {min_score_no_system:.1f}/10")
        print(f"â­ é«˜åˆ†(â‰¥8åˆ†): {high_scores_no_system}/{len(all_scores_no_system)} ({high_scores_no_system/len(all_scores_no_system)*100:.1f}%)")
        print(f"ğŸ“ˆ è¯„åˆ†åˆ†å¸ƒ: {[f'{s:.1f}' for s in all_scores_no_system]}\033[0m")
    
    print("=" * 80)
    
    return assessment_data

def save_assessment_results(persona_data, no_persona_data):
    """Save assessment results to parquet file"""
    # åˆå¹¶ä¸¤ç»„æ•°æ®
    all_data = persona_data + no_persona_data
    
    if not all_data:
        print("âŒ æ²¡æœ‰è¯„ä¼°æ•°æ®å¯ä¿å­˜")
        return
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(all_data)
    
    # ç¡®ä¿dataç›®å½•å­˜åœ¨
    data_dir = Path("data")
    data_dir.mkdir(exist_ok=True)
    
    # ä¿å­˜åˆ°parquetæ–‡ä»¶
    output_file = data_dir / "assessment.parquet"
    df.to_parquet(output_file, index=False)
    
    print(f"\nğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print(f"ğŸ“Š æ€»è®°å½•æ•°: {len(df)}")
    print(f"ğŸ“ˆ æ•°æ®åˆ—: {list(df.columns)}")
    
    # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
    with_persona = df[df['use_persona'] == True]
    without_persona = df[df['use_persona'] == False]
    
    print(f"\nğŸ“Š è¯¦ç»†æ•°æ®ç»Ÿè®¡:")
    print(f"ğŸ­ æœ‰äººè®¾æµ‹è¯•: {len(with_persona)}æ¡")
    print(f"ğŸ¤– æ— äººè®¾æµ‹è¯•: {len(without_persona)}æ¡")
    
    if len(with_persona) > 0:
        avg_with = with_persona['score'].mean()
        max_with = with_persona['score'].max()
        min_with = with_persona['score'].min()
        high_with = len(with_persona[with_persona['score'] >= 8.0])
        avg_time_with = with_persona['inference_time'].mean()
        
        print(f"\nğŸ­ æœ‰äººè®¾è¯¦ç»†ç»Ÿè®¡:")
        print(f"   ğŸ“Š å¹³å‡åˆ†: {avg_with:.2f}/10")
        print(f"   ğŸ” æœ€é«˜åˆ†: {max_with:.1f}/10")
        print(f"   ğŸ”» æœ€ä½åˆ†: {min_with:.1f}/10")
        print(f"   â­ é«˜åˆ†(â‰¥8åˆ†): {high_with}/{len(with_persona)} ({high_with/len(with_persona)*100:.1f}%)")
        print(f"   â±ï¸  å¹³å‡æ¨ç†æ—¶é—´: {avg_time_with:.2f}s")
    
    if len(without_persona) > 0:
        avg_without = without_persona['score'].mean()
        max_without = without_persona['score'].max()
        min_without = without_persona['score'].min()
        high_without = len(without_persona[without_persona['score'] >= 8.0])
        avg_time_without = without_persona['inference_time'].mean()
        
        print(f"\nğŸ¤– æ— äººè®¾è¯¦ç»†ç»Ÿè®¡:")
        print(f"   ğŸ“Š å¹³å‡åˆ†: {avg_without:.2f}/10")
        print(f"   ğŸ” æœ€é«˜åˆ†: {max_without:.1f}/10")
        print(f"   ğŸ”» æœ€ä½åˆ†: {min_without:.1f}/10")
        print(f"   â­ é«˜åˆ†(â‰¥8åˆ†): {high_without}/{len(without_persona)} ({high_without/len(without_persona)*100:.1f}%)")
        print(f"   â±ï¸  å¹³å‡æ¨ç†æ—¶é—´: {avg_time_without:.2f}s")
    
    if len(with_persona) > 0 and len(without_persona) > 0:
        print(f"\nğŸ“ˆ å¯¹æ¯”æ•ˆæœ:")
        print(f"   ğŸ”„ è¯„åˆ†æå‡: {avg_with - avg_without:+.2f}åˆ†")
        print(f"   â±ï¸  æ—¶é—´å·®å¼‚: {avg_time_with - avg_time_without:+.2f}s")
        print(f"   ğŸ“Š é«˜åˆ†ç‡æå‡: {high_with/len(with_persona)*100 - high_without/len(without_persona)*100:+.1f}%")
    
    # ä¿å­˜æ•°æ®å­—å…¸
    data_dict = {
        'columns': {
            'timestamp': 'æµ‹è¯•æ—¶é—´æˆ³',
            'question_id': 'é—®é¢˜ç¼–å·',
            'question': 'æµ‹è¯•é—®é¢˜',
            'response': 'æ¨¡å‹åŸå§‹å›å¤',
            'clean_response': 'æå–çš„åŠ©æ‰‹å›å¤å†…å®¹',
            'score': 'æš–ç”·äººè®¾è¯„åˆ†(1-10)',
            'reason': 'è¯„åˆ†è¯¦ç»†ç†ç”±',
            'inference_time': 'æ¨ç†è€—æ—¶(ç§’)',
            'use_persona': 'æ˜¯å¦ä½¿ç”¨äººè®¾ç³»ç»Ÿæç¤ºè¯',
            'test_type': 'æµ‹è¯•ç±»å‹',
            'system_prompt': 'ç³»ç»Ÿæç¤ºè¯å†…å®¹',
            'model_name': 'æ¨¡å‹åç§°',
            'server_url': 'æ¨ç†æœåŠ¡å™¨åœ°å€',
            'llama4_url': 'è¯„åˆ†æ¨¡å‹åœ°å€'
        },
        'test_summary': {
            'total_records': len(df),
            'with_persona_count': len(with_persona),
            'without_persona_count': len(without_persona),
            'avg_score_with_persona': avg_with if len(with_persona) > 0 else None,
            'avg_score_without_persona': avg_without if len(without_persona) > 0 else None,
            'score_improvement': avg_with - avg_without if len(with_persona) > 0 and len(without_persona) > 0 else None
        }
    }
    
    # ä¿å­˜æ•°æ®å­—å…¸åˆ°JSON
    dict_file = data_dir / "assessment_metadata.json"
    with open(dict_file, 'w', encoding='utf-8') as f:
        json.dump(data_dict, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“‹ æ•°æ®å­—å…¸å·²ä¿å­˜åˆ°: {dict_file}")

def run_complete_assessment():
    """Run complete assessment with both persona and no-persona tests"""
    print("ğŸš€ å¼€å§‹å®Œæ•´è¯„ä¼°æµ‹è¯•...")
    
    # è¿è¡Œæœ‰äººè®¾æµ‹è¯•
    persona_data = run_test_questions()
    
    # è¿è¡Œæ— äººè®¾æµ‹è¯•  
    no_persona_data = run_no_persona_tests()
    
    # å¯¹æ¯”åˆ†æ
    if persona_data and no_persona_data:
        persona_scores = [d['score'] for d in persona_data if d['score'] > 0]
        no_persona_scores = [d['score'] for d in no_persona_data if d['score'] > 0]
        
        if persona_scores and no_persona_scores:
            avg_with_persona = sum(persona_scores) / len(persona_scores)
            avg_without_persona = sum(no_persona_scores) / len(no_persona_scores)
            difference = avg_with_persona - avg_without_persona
            
            print(f"\n\033[91mğŸ“ˆ å¯¹æ¯”åˆ†æ:")
            print(f"ğŸ”„ æœ‰/æ— ç³»ç»Ÿæç¤ºè¯å¹³å‡åˆ†å·®: {difference:+.2f}")
            if difference > 1.0:
                print(f"ğŸ“Š ç³»ç»Ÿæç¤ºè¯æ•ˆæœ: æ˜¾è‘—æå‡")
            elif difference > 0.5:
                print(f"ğŸ“Š ç³»ç»Ÿæç¤ºè¯æ•ˆæœ: æ˜æ˜¾æå‡")
            elif difference > 0:
                print(f"ğŸ“Š ç³»ç»Ÿæç¤ºè¯æ•ˆæœ: è½»å¾®æå‡")
            elif difference > -0.5:
                print(f"ğŸ“Š ç³»ç»Ÿæç¤ºè¯æ•ˆæœ: åŸºæœ¬æ— å·®å¼‚")
            else:
                print(f"ğŸ“Š ç³»ç»Ÿæç¤ºè¯æ•ˆæœ: å¯èƒ½äº§ç”Ÿè´Ÿé¢å½±å“")
            print("\033[0m")
    
    # ä¿å­˜ç»“æœ
    save_assessment_results(persona_data, no_persona_data)
    
    return persona_data, no_persona_data

def main():
    parser = argparse.ArgumentParser(description="Qwen Omni Inference Client")
    parser.add_argument("--server", default="http://localhost:10020", help="Server URL")
    parser.add_argument(
        "--text",
        default='Hello, how are you?',
        required=False,
        help="Text input for inference"
    )
    parser.add_argument("--image", help="Path to image file (optional)")
    parser.add_argument("--check-health", action="store_true", help="Check server health")
    parser.add_argument("--test", action="store_false", help="Run test questions")
    parser.add_argument("--assessment", action="store_false", help="Run complete assessment and save to parquet")
    
    args = parser.parse_args()
    
    if args.assessment:
        run_complete_assessment()
    elif args.test:
        run_test_questions()
    else:
        client = InferenceClient(args.server)
    
    # Health check if requested
    if args.check_health:
        print("ğŸ” Checking server health...")
        health = client.health_check()
        print(f"Health status: {json.dumps(health, indent=2)}")
        if health.get('status') != 'healthy':
            print("âŒ Server is not healthy!")
            return
        print("âœ… Server is healthy!")
        print()
    
    # Run test questions if requested
    if args.test:
        run_test_questions(client)
        return
    
    # Perform inference
    print(f"ğŸ“ Text input: {args.text}")
    if args.image:
        print(f"ğŸ–¼ï¸  Image: {args.image}")
        if not Path(args.image).exists():
            print(f"âŒ Image file not found: {args.image}")
            return
        
        result = client.multimodal_inference(args.text, args.image)
    else:
        result = client.text_inference(args.text)
    
    # Display results
    print("=" * 60)
    if "error" in result:
        print(f"âŒ Error: {result['error']}")
    else:
        print("âœ… Inference successful!")
        print(f"\033[92mğŸ¤– Response: {result.get('response', 'No response')}\033[0m")  # ç»¿è‰²å“åº”
        
        # Show timing summary
        if 'inference_time' in result:
            print(f"â±ï¸  Total time: {result['inference_time']:.3f}s")
    print("=" * 60)

if __name__ == "__main__":
    main()
