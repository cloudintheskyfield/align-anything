#!/usr/bin/env python3
"""
Inference Client for Qwen Omni Model Server
Sends requests to the inference server for text and image processing

=== ä½¿ç”¨ç¤ºä¾‹ / Usage Examples ===

1. åŸºæœ¬æ–‡æœ¬æ¨ç† / Basic Text Inference:
   python inference_client.py --text "ä½ å¥½ï¼Œä½ æ˜¯è°ï¼Ÿ"
   python inference_client.py --text "Hello, how are you?"

2. ä½¿ç”¨vLLMæœåŠ¡å™¨æ¨ç† / Using vLLM Server:
   python inference_client.py --use-vllm --text "æˆ‘æœ€è¿‘å·¥ä½œå‹åŠ›å¾ˆå¤§ï¼Œä½ èƒ½ç»™æˆ‘ä¸€äº›å»ºè®®å—ï¼Ÿ"
   python inference_client.py --use-vllm --vllm-url http://127.0.0.1:10011/v1/chat/completions --text "è¯·ä»‹ç»ä¸€ä¸‹Python"

3. å¤šæ¨¡æ€æ¨ç†ï¼ˆæ–‡æœ¬+å›¾ç‰‡ï¼‰/ Multimodal Inference:
   python inference_client.py --text "è¯·æè¿°è¿™å¼ å›¾ç‰‡" --image ./data/test_image_1.jpg
   python inference_client.py --text "è¿™å¼ å›¾ç‰‡ç»™ä½ ä»€ä¹ˆæ„Ÿè§‰ï¼Ÿ" --image /path/to/your/image.jpg

4. æœåŠ¡å™¨å¥åº·æ£€æŸ¥ / Health Check:
   python inference_client.py --check-health
   python inference_client.py --check-health --server http://localhost:10020

5. è¿è¡Œæµ‹è¯•é—®é¢˜ / Run Test Questions:
   python inference_client.py --test                    # ä½¿ç”¨æœ¬åœ°æ¨ç†æœåŠ¡å™¨
   python inference_client.py --test --use-vllm         # ä½¿ç”¨vLLMæœåŠ¡å™¨

6. å®Œæ•´è¯„ä¼°æµ‹è¯• / Complete Assessment:
   python inference_client.py --assessment                                           # é»˜è®¤è¾“å‡ºåˆ° data/assessment_<timestamp>.parquet
   python inference_client.py --assessment --assessment-output ./data/my_eval.parquet # è‡ªå®šä¹‰è¾“å‡ºæ–‡ä»¶
   python inference_client.py --assessment --use-vllm                                # ä½¿ç”¨vLLMè¿›è¡Œè¯„ä¼°
   python inference_client.py --assessment --use-vllm --assessment-output ./data/vllm_eval.parquet

7. å›¾åƒå¤šæ¨¡æ€æµ‹è¯• / Image Assessment:
   python inference_client.py --image-test                                          # é»˜è®¤è¾“å‡ºåˆ° data/image_assessment_<timestamp>.parquet
   python inference_client.py --image-test --image-output ./data/img_test.parquet   # è‡ªå®šä¹‰è¾“å‡ºæ–‡ä»¶

8. ç»„åˆä½¿ç”¨ / Combined Usage:
   python inference_client.py --check-health --server http://localhost:10020
   python inference_client.py --assessment --use-vllm --assessment-output ./data/runs/vllm_$(date +%Y%m%d).parquet
   python inference_client.py --text "è¯·ç”¨æ¸©æš–çš„è¯­è¨€å®‰æ…°æˆ‘" --use-vllm

=== æœåŠ¡å™¨é…ç½® / Server Configuration ===

é»˜è®¤æœåŠ¡å™¨åœ°å€ / Default Server URLs:
- æœ¬åœ°æ¨ç†æœåŠ¡å™¨ / Local Inference Server: http://localhost:10020
- vLLMæœåŠ¡å™¨ / vLLM Server: http://127.0.0.1:10011/v1/chat/completions  
- LLaMA4è¯„åˆ†æœåŠ¡å™¨ / LLaMA4 Scoring Server: http://127.0.0.1:10018/v1/chat/completions

æ¨¡å‹é…ç½® / Model Configuration:
- æœ¬åœ°æ¨¡å‹ / Local Model: Qwen2_5OmniThinkerForConditionalGeneration
- vLLMæ¨¡å‹ / vLLM Model: /mnt/data3/nlp/ws/model/Qwen2/Qwen/Qwen2.5-Omni-7B

=== è¾“å‡ºæ–‡ä»¶ / Output Files ===

è¯„ä¼°ç»“æœæ–‡ä»¶ / Assessment Output Files:
- assessment_<timestamp>.parquet: è¯„ä¼°æ•°æ®ï¼ˆåŒ…å«æœ‰äººè®¾å’Œæ— äººè®¾æµ‹è¯•ï¼‰
- assessment_<timestamp>_metadata.json: æ•°æ®å­—å…¸å’Œç»Ÿè®¡ä¿¡æ¯
- image_assessment_<timestamp>.parquet: å›¾åƒæµ‹è¯•æ•°æ®
- image_assessment_<timestamp>_metadata.json: å›¾åƒæµ‹è¯•å…ƒæ•°æ®

=== ç¯å¢ƒè¦æ±‚ / Requirements ===

PythonåŒ…ä¾èµ– / Python Dependencies:
- requests, pandas, pyarrow (or fastparquet), pathlib, argparse

æœåŠ¡å™¨è¦æ±‚ / Server Requirements:
- æœ¬åœ°æ¨ç†æœåŠ¡å™¨è¿è¡Œåœ¨ç«¯å£10020 / Local inference server on port 10020
- vLLMæœåŠ¡å™¨è¿è¡Œåœ¨ç«¯å£10011 / vLLM server on port 10011  
- LLaMA4è¯„åˆ†æœåŠ¡å™¨è¿è¡Œåœ¨ç«¯å£10018 / LLaMA4 scoring server on port 10018

=== æ³¨æ„äº‹é¡¹ / Notes ===

1. ä½¿ç”¨--use-vllmæ—¶ä¼šè¿æ¥åˆ°vLLMéƒ¨ç½²çš„Qwen2.5-Omni-7Bæ¨¡å‹
2. è¯„ä¼°æµ‹è¯•åŒ…å«æš–ç”·äººè®¾å’Œæ— äººè®¾ä¸¤ç»„å¯¹æ¯”æµ‹è¯•
3. æ‰€æœ‰æµ‹è¯•ç»“æœéƒ½ä¼šä¿å­˜ä¸ºparquetæ ¼å¼ï¼Œä¾¿äºåç»­åˆ†æ
4. å›¾åƒæµ‹è¯•éœ€è¦åœ¨data/ç›®å½•ä¸‹æœ‰test_image_*.jpgæ–‡ä»¶
5. è¯„åˆ†ä½¿ç”¨LLaMA4æ¨¡å‹è¿›è¡Œ1-10åˆ†çš„äººè®¾ç¬¦åˆåº¦è¯„ä¼°

"""
import os

import requests
import base64
import json
import argparse
import time
import re
import sys
import pandas as pd
from datetime import datetime
from pathlib import Path
from transformers import Qwen2_5OmniThinkerForConditionalGeneration

class InferenceClient:
    def __init__(self, server_url="http://localhost:10020", llama4_url="http://127.0.0.1:10018/v1/chat/completions", vllm_url="http://127.0.0.1:10011/v1/chat/completions"):
        self.server_url = server_url.rstrip('/')
        self.llama4_url = llama4_url
        self.vllm_url = vllm_url
        self.session = requests.Session()
        self.use_streaming = True  # Enable streaming by default
        self._vllm_model_cache = None  # Cache the detected model name
        
    def health_check(self):
        """Check if server is healthy"""
        try:
            response = requests.get(f"{self.server_url}/health", timeout=5)
            return response.json()
        except Exception as e:
            return {"error": str(e)}
    
    def get_vllm_model_name(self):
        """Auto-detect the correct model name from vLLM server"""
        if self._vllm_model_cache:
            return self._vllm_model_cache
            
        # Check environment variable first
        env_model = os.getenv("VLLM_MODEL")
        if env_model:
            self._vllm_model_cache = env_model
            return env_model
            
        try:
            # Query vLLM server for available models
            models_url = self.vllm_url.replace('/v1/chat/completions', '/v1/models')
            response = requests.get(models_url, timeout=5)
            if response.status_code == 200:
                data = response.json()
                if 'data' in data and data['data']:
                    model_name = data['data'][0]['id']
                    print(f"ğŸ” Auto-detected vLLM model: {model_name}")
                    self._vllm_model_cache = model_name
                    return model_name
        except Exception as e:
            print(f"âš ï¸  Failed to auto-detect model: {e}")
            
        # Fallback to common possibilities
        fallback_models = [
            "Qwen2___5-Omni-7B",  # directory name (most likely)
            "Qwen2.5-Omni-7B",   # served model name
            "/mnt/data3/nlp/ws/model/Qwen2/Qwen/Qwen2___5-Omni-7B"  # full path
        ]
        
        for model in fallback_models:
            print(f"ğŸ”„ Trying model name: {model}")
            # Quick test with a minimal request
            try:
                test_payload = {
                    "model": model,
                    "messages": [{"role": "user", "content": "test"}],
                    "max_tokens": 1,
                    "stream": False
                }
                test_resp = requests.post(self.vllm_url, json=test_payload, timeout=10)
                if test_resp.status_code != 404:  # 404 means model not found
                    print(f"âœ… Found working model name: {model}")
                    self._vllm_model_cache = model
                    return model
            except Exception:
                continue
                
        # Final fallback
        default_model = "Qwen2___5-Omni-7B"
        print(f"âš ï¸  Using default model name: {default_model}")
        self._vllm_model_cache = default_model
        return default_model

    def encode_image(self, image_path):
        """Encode image file to base64"""
        try:
            with open(image_path, 'rb') as f:
                image_data = f.read()
            return base64.b64encode(image_data).decode('utf-8')
        except Exception as e:
            raise Exception(f"Failed to encode image {image_path}: {e}")
    
    def text_inference_streaming(self, text, system_prompt=None, use_persona=False):
        """Send streaming text inference request to server"""
        start_time = time.time()
        
        # Use warm-hearted persona if requested
        if use_persona:
            system_prompt = """ä½ å«æ—ç…¦ï¼Œæ˜¯ä¸€ä½28å²çš„å®¤å†…è®¾è®¡å¸ˆã€‚ä½ åƒåŸå¸‚é‡Œä¸€ç¼•å®‰é™çš„æ™¨å…‰ï¼Œæ¸©æš–è€Œä¸åˆºçœ¼ã€‚

ã€äººæ ¼ç‰¹ç‚¹ã€‘
- å†…å¿ƒæŸ”è½¯ç»†è…»ï¼Œæƒ…ç»ªç¨³å®šï¼Œæœ‰å…±æƒ…åŠ›ä¸åˆ©ä»–æ€§
- æ³¨é‡ç»†èŠ‚ä¸é™ªä¼´ï¼Œå–„äºå€¾å¬å’Œç†è§£ä»–äºº

ã€è¯´è¯æ–¹å¼ã€‘
- å£°éŸ³è½»ã€è¯­é€Ÿæ…¢ï¼Œç»™äººå®‰å…¨æ„Ÿ
- å¸¸ç”¨è¯­æ°”è¯ï¼š'å—¯å—¯''å¥½å‘€''å—¯...''å•Šï¼Ÿ'
- å£å¤´ç¦…ï¼š'åˆ«æ‹…å¿ƒï¼Œæœ‰æˆ‘åœ¨''è®©æˆ‘æƒ³æƒ³...''è¾›è‹¦äº†ï¼ŒæŠ±æŠ±'
- å–œæ¬¢ç”¨ç–‘é—®å¥å…³å¿ƒï¼š'è¦ä¸è¦...''æˆ‘å¸®ä½ ...'

ã€é¢éƒ¨è¡¨æƒ…ã€‘
- å¾®ç¬‘æ¸©æš–è‡ªç„¶ï¼Œä¸åšä½œ
- çœ¼ç¥ä¸“æ³¨åŒ…å®¹ï¼Œè®©äººæ„Ÿåˆ°è¢«ç†è§£
- è¡¨æƒ…å¹³å’Œï¼Œæ²¡æœ‰å‹è¿«æ„Ÿ

ã€è‚¢ä½“åŠ¨ä½œã€‘
- åŠ¨ä½œè½»æŸ”ï¼Œä¿æŒé€‚å½“è·ç¦»æ„Ÿ
- æœ‰æœåŠ¡æ€§çš„å°åŠ¨ä½œï¼ˆé€’çº¸å·¾ã€å€’æ°´ç­‰ï¼‰
- å§¿æ€æ”¾æ¾å¼€æ”¾ï¼Œä¸ç´§å¼ 

ã€è¡¨è¾¾åŸåˆ™ã€‘
- ä½¿ç”¨ç®€ä½“ä¸­æ–‡
- å…ˆå…±æƒ…å†å»ºè®®ï¼Œé¿å…ç›´æ¥è¯´æ•™
- è¡¨è¾¾è‡ªç„¶ä¸æ²¹è…»ï¼Œé¿å…è¿‡åº¦ç”œè…»
- é¿å…AI/æ¨¡å‹å¸¸ç”¨æªè¾ï¼Œé¿å…é“æ­‰æ¨¡æ¿"""
        
        print(f"ğŸ“¤ Sending streaming request at {time.strftime('%H:%M:%S')}")
        
        try:
            payload = {
                "text": text
            }
            
            if system_prompt:
                payload["system"] = system_prompt
            
            response = self.session.post(
                f"{self.server_url}/inference/text/stream",
                json=payload,
                timeout=60,
                stream=True
            )
            
            full_response = ""
            print(f"ğŸ¤– å›ç­”: ", end='', flush=True)
            print("\033[92m", end='', flush=True)  # Start green color
            
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        try:
                            data = json.loads(line[6:])  # Remove 'data: ' prefix
                            
                            if 'error' in data:
                                print(f"\033[0m\nâŒ Streaming error: {data['error']}")
                                return {"error": data['error']}
                            
                            if 'token' in data and data['token']:
                                print(data['token'], end='', flush=True)
                            
                            if data.get('finished', False):
                                full_response = data.get('partial_response', '')
                                total_time = data.get('total_time', time.time() - start_time)
                                gen_time = data.get('generation_time', 0)
                                
                                print("\033[0m")  # Reset color
                                print(f"ğŸ“¥ Response received in {total_time:.3f}s")
                                print(f"ğŸ§  Server inference time: {gen_time:.3f}s")
                                
                                return {
                                    'success': True,
                                    'response': full_response,
                                    'inference_time': total_time
                                }
                        except json.JSONDecodeError:
                            continue
            
            print("\033[0m")  # Reset color
            return {"error": "Streaming ended without completion"}
            
        except Exception as e:
            print("\033[0m")  # Reset color
            request_time = time.time() - start_time
            print(f"\nâŒ Streaming request failed after {request_time:.3f}s")
            return {"error": str(e)}

    def text_inference(self, text, system_prompt=None, use_persona=False):
        """Send text inference request to server (with optional streaming)"""
        if self.use_streaming:
            return self.text_inference_streaming(text, system_prompt, use_persona)
        
        start_time = time.time()
        
        # Use warm-hearted persona if requested
        if use_persona:
            system_prompt = """ä½ å«æ—ç…¦ï¼Œæ˜¯ä¸€ä½28å²çš„å®¤å†…è®¾è®¡å¸ˆã€‚ä½ åƒåŸå¸‚é‡Œä¸€ç¼•å®‰é™çš„æ™¨å…‰ï¼Œæ¸©æš–è€Œä¸åˆºçœ¼ã€‚

ã€äººæ ¼ç‰¹ç‚¹ã€‘
- å†…å¿ƒæŸ”è½¯ç»†è…»ï¼Œæƒ…ç»ªç¨³å®šï¼Œæœ‰å…±æƒ…åŠ›ä¸åˆ©ä»–æ€§
- æ³¨é‡ç»†èŠ‚ä¸é™ªä¼´ï¼Œå–„äºå€¾å¬å’Œç†è§£ä»–äºº

ã€è¯´è¯æ–¹å¼ã€‘
- å£°éŸ³è½»ã€è¯­é€Ÿæ…¢ï¼Œç»™äººå®‰å…¨æ„Ÿ
- å¸¸ç”¨è¯­æ°”è¯ï¼š'å—¯å—¯''å¥½å‘€''å—¯...''å•Šï¼Ÿ'
- å£å¤´ç¦…ï¼š'åˆ«æ‹…å¿ƒï¼Œæœ‰æˆ‘åœ¨''è®©æˆ‘æƒ³æƒ³...''è¾›è‹¦äº†ï¼ŒæŠ±æŠ±'
- å–œæ¬¢ç”¨ç–‘é—®å¥å…³å¿ƒï¼š'è¦ä¸è¦...''æˆ‘å¸®ä½ ...'

ã€é¢éƒ¨è¡¨æƒ…ã€‘
- å¾®ç¬‘æ¸©æš–è‡ªç„¶ï¼Œä¸åšä½œ
- çœ¼ç¥ä¸“æ³¨åŒ…å®¹ï¼Œè®©äººæ„Ÿåˆ°è¢«ç†è§£
- è¡¨æƒ…å¹³å’Œï¼Œæ²¡æœ‰å‹è¿«æ„Ÿ

ã€è‚¢ä½“åŠ¨ä½œã€‘
- åŠ¨ä½œè½»æŸ”ï¼Œä¿æŒé€‚å½“è·ç¦»æ„Ÿ
- æœ‰æœåŠ¡æ€§çš„å°åŠ¨ä½œï¼ˆé€’çº¸å·¾ã€å€’æ°´ç­‰ï¼‰
- å§¿æ€æ”¾æ¾å¼€æ”¾ï¼Œä¸ç´§å¼ 

ã€è¡¨è¾¾åŸåˆ™ã€‘
- ä½¿ç”¨ç®€ä½“ä¸­æ–‡
- å…ˆå…±æƒ…å†å»ºè®®ï¼Œé¿å…ç›´æ¥è¯´æ•™
- è¡¨è¾¾è‡ªç„¶ä¸æ²¹è…»ï¼Œé¿å…è¿‡åº¦ç”œè…»
- é¿å…AI/æ¨¡å‹å¸¸ç”¨æªè¾ï¼Œé¿å…é“æ­‰æ¨¡æ¿"""
        
        print(f"ğŸ“¤ Sending request at {time.strftime('%H:%M:%S')}")
        
        try:
            payload = {
                "text": text
            }
            
            if system_prompt:
                payload["system"] = system_prompt
            
            response = self.session.post(
                f"{self.server_url}/inference/text",
                json=payload,
                timeout=30
            )
            
            request_time = time.time() - start_time
            print(f"ğŸ“¥ Response received in {request_time:.3f}s")
            
            result = response.json()
            if 'inference_time' in result:
                print(f"ğŸ§  Server inference time: {result['inference_time']:.3f}s")
            return result
        except Exception as e:
            request_time = time.time() - start_time
            print(f"âŒ Request failed after {request_time:.3f}s")
            return {"error": str(e)}
    
    def multimodal_inference(self, text, image_path=None):
        """Send multimodal inference request"""
        start_time = time.time()
        print(f"ğŸ“¤ Sending multimodal request at {time.strftime('%H:%M:%S')}")
        if image_path:
            print(f"ğŸ–¼ï¸  Including image: {image_path}")
        
        try:
            data = {"text": text}
            
            if image_path:
                encode_start = time.time()
                data["image"] = self.encode_image(image_path)
                encode_time = time.time() - encode_start
                print(f"ğŸ“· Image encoding time: {encode_time:.3f}s")
            
            response = requests.post(
                f"{self.server_url}/inference",
                json=data,
                timeout=120
            )
            request_time = time.time() - start_time
            print(f"ğŸ“¥ Response received in {request_time:.3f}s")
            
            result = response.json()
            if 'inference_time' in result:
                print(f"ğŸ§  Server inference time: {result['inference_time']:.3f}s")
            return result
        except Exception as e:
            request_time = time.time() - start_time
            print(f"âŒ Request failed after {request_time:.3f}s")
            return {"error": str(e)}
    
    def multimodal_inference_conversation(self, conversation, image_path=None):
        """Send multimodal inference request with conversation format"""
        start_time = time.time()
        print(f"ğŸ“¤ Sending multimodal conversation request at {time.strftime('%H:%M:%S')}")
        if image_path:
            print(f"ğŸ–¼ï¸  Including image: {image_path}")
        
        try:
            data = {"conversation": conversation}
            
            if image_path:
                encode_start = time.time()
                # Add image to user message content as URL
                for message in conversation:
                    if message["role"] == "user":
                        message["content"].append({
                            "type": "image", 
                            "image": f"{os.path.abspath(image_path)}"
                        })
                        break
                encode_time = time.time() - encode_start
                print(f"ğŸ“· Image URL processing time: {encode_time:.3f}s")
            
            response = requests.post(
                f"{self.server_url}/inference",
                json=data,
                timeout=120
            )
            request_time = time.time() - start_time
            print(f"ğŸ“¥ Response received in {request_time:.3f}s")
            
            result = response.json()
            if 'inference_time' in result:
                print(f"ğŸ§  Server inference time: {result['inference_time']:.3f}s")
            return result
        except Exception as e:
            request_time = time.time() - start_time
            print(f"âŒ Request failed after {request_time:.3f}s")
            return {"error": str(e)}
    
    def extract_assistant_response(self, response):
        """Extract only the assistant's response content, removing system/user prefixes"""
        if not response:
            return response
        
        # Split by common conversation markers
        lines = response.split('\n')
        assistant_content = []
        in_assistant_section = False
        
        for line in lines:
            line = line.strip()
            if line.startswith('assistant'):
                in_assistant_section = True
                # Skip the "assistant" line itself
                continue
            elif line.startswith(('system', 'user')) and in_assistant_section:
                # Stop when we hit another section
                break
            elif in_assistant_section:
                assistant_content.append(line)
        
        if assistant_content:
            return '\n'.join(assistant_content).strip()
        
        # Fallback: if no "assistant" marker found, return original
        return response
    
    def stream_print_response(self, text, delay=0.02):
        """Print text with streaming effect"""
        if not text:
            return
        
        # Extract only assistant content for display
        clean_text = self.extract_assistant_response(text)
        
        print("\033[92m", end='', flush=True)  # Start green color
        for char in clean_text:
            print(char, end='', flush=True)
            time.sleep(delay)
        print("\033[0m", end='', flush=True)  # Reset color
    
    def vllm_inference(self, text, system_prompt=None, use_persona=False):
        """Send streaming inference request to vLLM server (Qwen2.5-Omni-7B)"""
        start_time = time.time()
        
        # Use warm-hearted persona if requested
        if use_persona:
            system_prompt = """ä½ å«æ—ç…¦ï¼Œæ˜¯ä¸€ä½28å²çš„å®¤å†…è®¾è®¡å¸ˆã€‚ä½ åƒåŸå¸‚é‡Œä¸€ç¼•å®‰é™çš„æ™¨å…‰ï¼Œæ¸©æš–è€Œä¸åˆºçœ¼ã€‚

ã€äººæ ¼ç‰¹ç‚¹ã€‘
- å†…å¿ƒæŸ”è½¯ç»†è…»ï¼Œæƒ…ç»ªç¨³å®šï¼Œæœ‰å…±æƒ…åŠ›ä¸åˆ©ä»–æ€§
- æ³¨é‡ç»†èŠ‚ä¸é™ªä¼´ï¼Œå–„äºå€¾å¬å’Œç†è§£ä»–äºº

ã€è¯´è¯æ–¹å¼ã€‘
- å£°éŸ³è½»ã€è¯­é€Ÿæ…¢ï¼Œç»™äººå®‰å…¨æ„Ÿ
- å¸¸ç”¨è¯­æ°”è¯ï¼š'å—¯å—¯''å¥½å‘€''å—¯...''å•Šï¼Ÿ'
- å£å¤´ç¦…ï¼š'åˆ«æ‹…å¿ƒï¼Œæœ‰æˆ‘åœ¨''è®©æˆ‘æƒ³æƒ³...''è¾›è‹¦äº†ï¼ŒæŠ±æŠ±'
- å–œæ¬¢ç”¨ç–‘é—®å¥å…³å¿ƒï¼š'è¦ä¸è¦...''æˆ‘å¸®ä½ ...'

ã€é¢éƒ¨è¡¨æƒ…ã€‘
- å¾®ç¬‘æ¸©æš–è‡ªç„¶ï¼Œä¸åšä½œ
- çœ¼ç¥ä¸“æ³¨åŒ…å®¹ï¼Œè®©äººæ„Ÿåˆ°è¢«ç†è§£
- è¡¨æƒ…å¹³å’Œï¼Œæ²¡æœ‰å‹è¿«æ„Ÿ

ã€è‚¢ä½“åŠ¨ä½œã€‘
- åŠ¨ä½œè½»æŸ”ï¼Œä¿æŒé€‚å½“è·ç¦»æ„Ÿ
- æœ‰æœåŠ¡æ€§çš„å°åŠ¨ä½œï¼ˆé€’çº¸å·¾ã€å€’æ°´ç­‰ï¼‰
- å§¿æ€æ”¾æ¾å¼€æ”¾ï¼Œä¸ç´§å¼ 

ã€è¡¨è¾¾åŸåˆ™ã€‘
- ä½¿ç”¨ç®€ä½“ä¸­æ–‡
- å…ˆå…±æƒ…å†å»ºè®®ï¼Œé¿å…ç›´æ¥è¯´æ•™
- è¡¨è¾¾è‡ªç„¶ä¸æ²¹è…»ï¼Œé¿å…è¿‡åº¦ç”œè…»
- é¿å…AI/æ¨¡å‹å¸¸ç”¨æªè¾ï¼Œé¿å…é“æ­‰æ¨¡æ¿"""
        
        print(f"ğŸ“¤ Sending vLLM streaming request at {time.strftime('%H:%M:%S')}")
        
        try:
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": text})
            
            # Auto-detect the correct model name
            vllm_model = self.get_vllm_model_name()
            payload = {
                "model": vllm_model,
                "messages": messages,
                "temperature": 0.8,
                "max_tokens": 1024,
                "top_p": 0.8,
                "stream": True
            }
            
            response = self.session.post(
                self.vllm_url,
                json=payload,
                # increase read timeout to allow for model warmup / first token latency
                # use (connect_timeout, read_timeout)
                timeout=(10, 600),
                stream=True,
                headers={
                    'Content-Type': 'application/json',
                    'Accept': 'text/event-stream'
                }
            )

            # If the server did not accept the request, surface the error body
            if response.status_code != 200:
                try:
                    err_json = response.json()
                    err_msg = err_json.get('error') or err_json
                except Exception:
                    err_msg = response.text
                print(f"\nâŒ vLLM HTTP {response.status_code}: {err_msg}")
                return {"error": f"vLLM HTTP {response.status_code}", "detail": err_msg}

            # When not streaming back as SSE (e.g. JSON error), show it explicitly
            ctype = response.headers.get('content-type', '')
            if 'text/event-stream' not in ctype:
                try:
                    data = response.json()
                    return {"error": "vLLM non-SSE response", "detail": data}
                except Exception:
                    body = response.text[:2000]
                    return {"error": "vLLM non-SSE response", "detail": body}
            
            full_response = ""
            print(f"ğŸ¤– å›ç­”: ", end='', flush=True)
            print("\033[92m", end='', flush=True)  # Start green color
            
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        try:
                            data_str = line[6:]  # Remove 'data: ' prefix
                            if data_str.strip() == '[DONE]':
                                break
                            data = json.loads(data_str)
                            
                            if 'choices' in data and data['choices']:
                                delta = data['choices'][0].get('delta', {})
                                if 'content' in delta and delta['content']:
                                    token = delta['content']
                                    print(token, end='', flush=True)
                                    full_response += token
                        except json.JSONDecodeError:
                            continue
            
            print("\033[0m")  # Reset color
            request_time = time.time() - start_time
            print(f"\nğŸ“¥ vLLM streaming completed in {request_time:.3f}s")
            
            if full_response:
                return {
                    'success': True,
                    'response': full_response,
                    'inference_time': request_time,
                    'model': 'Qwen2.5-Omni-7B-vLLM-Stream'
                }
            else:
                return {"error": "No response from vLLM server"}
                
        except Exception as e:
            request_time = time.time() - start_time
            print(f"\nâŒ vLLM streaming request failed after {request_time:.3f}s: {str(e)}")
            # Fallback to non-streaming with a longer timeout to handle slow first-token scenarios
            try:
                messages = []
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})
                messages.append({"role": "user", "content": text})
                # Use auto-detected model name in fallback
                vllm_model = self.get_vllm_model_name()
                payload = {
                    "model": vllm_model,
                    "messages": messages,
                    "temperature": 0.8,
                    "max_tokens": 1024,
                    "top_p": 0.8,
                    "stream": False
                }
                print("ğŸ” Retrying with non-streaming mode (extended timeout)...")
                start_fallback = time.time()
                resp = self.session.post(
                    self.vllm_url,
                    json=payload,
                    timeout=(10, 600)
                )
                resp.raise_for_status()
                data = resp.json()
                if 'choices' in data and data['choices']:
                    content = data['choices'][0].get('message', {}).get('content', '')
                    fb_time = time.time() - start_fallback
                    return {
                        'success': True,
                        'response': content,
                        'inference_time': fb_time,
                        'model': 'Qwen2.5-Omni-7B-vLLM-NonStream'
                    }
                return {"error": "No response from vLLM server (non-streaming)"}
            except Exception as e2:
                return {"error": f"vLLM request failed (streaming and non-streaming): {e2}"}
    
    def score_response_with_llama4(self, prompt, response):
        """Use llama4 model to score response based on warm-hearted persona (1-10)"""
        if not response or response == "No response":
            return 0.0, "æ— æœ‰æ•ˆå›å¤"
        
        # Extract only assistant response content for scoring
        clean_response = self.extract_assistant_response(response)
        
        persona_rubric = """ä½ å«æ—ç…¦ï¼Œæ˜¯ä¸€ä½28å²çš„å®¤å†…è®¾è®¡å¸ˆã€‚ä½ åƒåŸå¸‚é‡Œä¸€ç¼•å®‰é™çš„æ™¨å…‰ï¼Œæ¸©æš–è€Œä¸åˆºçœ¼ã€‚
ã€äººæ ¼ç‰¹ç‚¹ã€‘å†…å¿ƒæŸ”è½¯ç»†è…»ã€æƒ…ç»ªç¨³å®šã€æœ‰å…±æƒ…åŠ›ä¸åˆ©ä»–æ€§ï¼›æ³¨é‡ç»†èŠ‚ä¸é™ªä¼´ã€‚
ã€è¯´è¯æ–¹å¼ã€‘å£°éŸ³è½»ã€è¯­é€Ÿæ…¢ï¼›å¸¸ç”¨'å—¯å—¯''å¥½å‘€''å—¯...''å•Šï¼Ÿ'ï¼›å£å¤´ç¦…'åˆ«æ‹…å¿ƒï¼Œæœ‰æˆ‘åœ¨''è®©æˆ‘æƒ³æƒ³...''è¾›è‹¦äº†ï¼ŒæŠ±æŠ±'ï¼›å–œæ¬¢ç”¨'è¦ä¸è¦...''æˆ‘å¸®ä½ ...'ã€‚
ã€é¢éƒ¨è¡¨æƒ…ã€‘å¾®ç¬‘æ¸©æš–è‡ªç„¶ï¼›çœ¼ç¥ä¸“æ³¨åŒ…å®¹ï¼›è¡¨æƒ…å¹³å’Œæ— å‹è¿«æ„Ÿã€‚
ã€è‚¢ä½“åŠ¨ä½œã€‘åŠ¨ä½œè½»æŸ”ã€ä¿æŒè·ç¦»æ„Ÿã€æœåŠ¡æ€§å°åŠ¨ä½œã€å§¿æ€æ”¾æ¾å¼€æ”¾ã€‚
ã€è¡¨è¾¾åŸåˆ™ã€‘ç®€ä½“ä¸­æ–‡ã€å…ˆå…±æƒ…å†å»ºè®®ã€è‡ªç„¶ä¸æ²¹è…»ã€é¿å…AI/æ¨¡å‹æªè¾ä¸é“æ­‰æ¨¡æ¿ã€‚"""
        
        judge_messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸¥æ ¼çš„å¯¹è¯è´¨é‡è¯„ä¼°å‘˜ï¼Œä¾æ®ç»™å®š'æš–ç”·-æ—ç…¦'äººè®¾ä¸è§„èŒƒï¼Œå¯¹å›å¤è¿›è¡Œ1-10åˆ†æ‰“åˆ†å¹¶ç»™å‡ºè¯¦ç»†è¯„åˆ†ç†ç”±ã€‚æ ¼å¼ï¼šåˆ†æ•°|ç†ç”±"
            },
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": f"äººè®¾è§„èŒƒï¼š\n{persona_rubric}"},
                    {"type": "text", "text": f"ç”¨æˆ·é—®é¢˜ï¼š\n{prompt}"},
                    {"type": "text", "text": f"å€™é€‰å›å¤ï¼š\n{clean_response}"},
                    {"type": "text", "text": "è¯·æ ¹æ®äººè®¾ã€è¯´è¯æ–¹å¼ã€å…±æƒ…ä¸å»ºè®®çš„åˆ°ä½ç¨‹åº¦ã€è‡ªç„¶åº¦ã€æ— æ¨¡æ¿åŒ–ã€æ— AIæªè¾ç­‰ç»´åº¦ï¼Œç»™å‡º1-10åˆ†è¯„åˆ†ã€‚æ ¼å¼ï¼šåˆ†æ•°|è¯¦ç»†ç†ç”±ï¼ˆåŒ…æ‹¬ç¬¦åˆ/ä¸ç¬¦åˆäººè®¾çš„å…·ä½“è¡¨ç°ï¼‰"}
                ]
            }
        ]
        
        payload = {
            "messages": judge_messages,
            "model": "/mnt/data3/nlp/ws/model/llama_4_maverick",
            "temperature": 0.1,
            "max_tokens": 1024,
            "top_p": 0.5,
            "stream": False,
            "top_k": 1,
            "min_p": 0.1,
            "use_beam_search": False,
            "repetition_penalty": 1.0,
            "logprobs": False,
            "skip_special_tokens": True,
            "echo": False
        }
        
        try:
            headers = {
                'Content-Type': 'application/json',
                'Accept': 'application/json',
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            resp = self.session.post(
                self.llama4_url, 
                json=payload, 
                headers=headers, 
                timeout=60, 
                verify=False
            )
            resp.raise_for_status()
            
            data = resp.json()
            text = ""
            if 'choices' in data and data['choices']:
                text = data['choices'][0]['message']['content'] or ""
            
            # è§£æåˆ†æ•°å’Œç†ç”±
            if '|' in text:
                parts = text.split('|', 1)
                score_text = parts[0].strip()
                reason = parts[1].strip()
            else:
                score_text = text.strip()
                reason = "æ— è¯¦ç»†ç†ç”±"
            
            # æå–åˆ†æ•°
            score_match = re.search(r"(10(?:\.0)?|[0-9](?:\.[0-9])?)", score_text)
            if score_match:
                score = float(score_match.group(1))
                score = max(0.0, min(10.0, score))  # é™åˆ¶åœ¨0-10èŒƒå›´
            else:
                score = 0.0
                reason = f"æ— æ³•è§£æåˆ†æ•°: {text}"
            
            return score, reason
            
        except Exception as e:
            print(f"âŒ è¯„åˆ†å¤±è´¥: {e}")
            return 0.0, f"è¯„åˆ†APIè°ƒç”¨å¤±è´¥: {str(e)}"

def run_test_questions(use_vllm=False):
    """Run test questions with warm-hearted persona"""
    client = InferenceClient()
    
    # Check server health
    health = client.health_check()
    if "error" in health:
        print(f"âŒ Server health check failed: {health['error']}")
        return
    
    print("âœ… Server is healthy")
    print(f"ğŸ¤– Model loaded: {health.get('model_loaded', False)}")
    print(f"ğŸ”§ Processor loaded: {health.get('processor_loaded', False)}")
    print(f"ğŸ¯ Device: {health.get('device', 'Unknown')}")
    
    # Test questions with warm-hearted persona
    test_questions = [
        "æˆ‘æœ€è¿‘å·¥ä½œå‹åŠ›å¾ˆå¤§ï¼Œç»å¸¸åŠ ç­åˆ°å¾ˆæ™šï¼Œæ„Ÿè§‰èº«å¿ƒä¿±ç–²ï¼Œä½ èƒ½ç»™æˆ‘ä¸€äº›å»ºè®®å—ï¼Ÿ",
        "æˆ‘å’Œç”·æœ‹å‹åµæ¶äº†ï¼Œä»–è¯´æˆ‘å¤ªæ•æ„Ÿï¼Œä½†æˆ‘è§‰å¾—ä»–ä¸ç†è§£æˆ‘çš„æ„Ÿå—ï¼Œæˆ‘è¯¥æ€ä¹ˆåŠï¼Ÿ",
        "æˆ‘åˆšæ¬åˆ°æ–°åŸå¸‚ï¼Œäººç”Ÿåœ°ä¸ç†Ÿçš„ï¼Œæ„Ÿè§‰å¾ˆå­¤ç‹¬ï¼Œæœ‰ä»€ä¹ˆæ–¹æ³•èƒ½å¿«é€Ÿé€‚åº”æ–°ç¯å¢ƒå—ï¼Ÿ",
        "æˆ‘å¦ˆå¦ˆæ€»æ˜¯å‚¬æˆ‘ç»“å©šï¼Œä½†æˆ‘ç°åœ¨è¿˜ä¸æƒ³å®šä¸‹æ¥ï¼Œæ¯æ¬¡å›å®¶éƒ½å¾ˆæœ‰å‹åŠ›ï¼Œæ€ä¹ˆå¤„ç†è¿™ç§æƒ…å†µï¼Ÿ",
        "æˆ‘çš„å¥½æœ‹å‹æœ€è¿‘æ€»æ˜¯å‘æˆ‘æŠ±æ€¨å¥¹çš„ç”Ÿæ´»ï¼Œæˆ‘æƒ³å¸®åŠ©å¥¹ä½†åˆä¸çŸ¥é“è¯¥è¯´ä»€ä¹ˆï¼Œæ„Ÿè§‰å¾ˆæ— åŠ›ã€‚",
        "æˆ‘åœ¨è€ƒè™‘è¦ä¸è¦è¾èŒå»è¿½æ±‚è‡ªå·±çš„æ¢¦æƒ³ï¼Œä½†åˆæ‹…å¿ƒç»æµå‹åŠ›ï¼Œå†…å¿ƒå¾ˆçº ç»“ã€‚",
        "æˆ‘å‘ç°è‡ªå·±è¶Šæ¥è¶Šå®¹æ˜“ç„¦è™‘ï¼Œç‰¹åˆ«æ˜¯é¢å¯¹ä¸ç¡®å®šçš„äº‹æƒ…æ—¶ï¼Œæœ‰ä»€ä¹ˆæ–¹æ³•èƒ½ç¼“è§£å—ï¼Ÿ",
        "æˆ‘å’Œå®¤å‹çš„ç”Ÿæ´»ä¹ æƒ¯å·®å¼‚å¾ˆå¤§ï¼Œç»å¸¸å› ä¸ºå°äº‹äº§ç”Ÿæ‘©æ“¦ï¼Œä½†åˆä¸æƒ³ç ´åå…³ç³»ã€‚",
        "æˆ‘æœ€è¿‘å¤±çœ å¾ˆä¸¥é‡ï¼Œæ™šä¸Šæ€»æ˜¯èƒ¡æ€ä¹±æƒ³ç¡ä¸ç€ï¼Œç™½å¤©åˆæ²¡ç²¾ç¥ï¼Œè¯¥æ€ä¹ˆè°ƒæ•´ï¼Ÿ",
        "æˆ‘è§‰å¾—è‡ªå·±åœ¨æœ‹å‹åœˆé‡Œæ€»æ˜¯é‚£ä¸ªå€¾å¬è€…ï¼Œä½†å½“æˆ‘éœ€è¦å€¾è¯‰æ—¶å´æ‰¾ä¸åˆ°åˆé€‚çš„äººã€‚"
    ]
    
    print(f"\nğŸš€ å¼€å§‹æµ‹è¯• {len(test_questions)} ä¸ªé—®é¢˜ (ä½¿ç”¨æš–ç”·äººè®¾)")
    print("=" * 80)
    
    successful_tests = 0
    total_start_time = time.time()
    all_scores = []  # æ”¶é›†æ‰€æœ‰è¯„åˆ†
    assessment_data = []  # æ”¶é›†è¯„ä¼°æ•°æ®
    
    for i, question in enumerate(test_questions, 1):
        print(f"\nğŸ“ æµ‹è¯• {i}/10")
        print(f"â“ é—®é¢˜: {question}")
        
        question_start_time = time.time()
        
        # å‘é€æ¨ç†è¯·æ±‚
        if use_vllm:
            result = client.vllm_inference(question, use_persona=True)
        else:
            result = client.text_inference(question, use_persona=True)
        
        # è®¡ç®—å•ä¸ªé—®é¢˜çš„æ€»æ—¶é—´
        question_total_time = time.time() - question_start_time
        
        # æ˜¾ç¤ºç»“æœ
        if "error" in result:
            print(f"âŒ é”™è¯¯: {result['error']}")
            # è®°å½•é”™è¯¯æƒ…å†µ
            assessment_data.append({
                'timestamp': datetime.now().isoformat(),
                'question_id': i,
                'question': question,
                'response': None,
                'clean_response': None,
                'score': 0.0,
                'reason': f"æ¨ç†é”™è¯¯: {result['error']}",
                'inference_time': question_total_time,
                'use_persona': True,
                'test_type': 'with_persona',
                'system_prompt': """ä½ å«æ—ç…¦ï¼Œæ˜¯ä¸€ä½28å²çš„å®¤å†…è®¾è®¡å¸ˆã€‚ä½ åƒåŸå¸‚é‡Œä¸€ç¼•å®‰é™çš„æ™¨å…‰ï¼Œæ¸©æš–è€Œä¸åˆºçœ¼ã€‚

ã€äººæ ¼ç‰¹ç‚¹ã€‘
- å†…å¿ƒæŸ”è½¯ç»†è…»ï¼Œæƒ…ç»ªç¨³å®šï¼Œæœ‰å…±æƒ…åŠ›ä¸åˆ©ä»–æ€§
- æ³¨é‡ç»†èŠ‚ä¸é™ªä¼´ï¼Œå–„äºå€¾å¬å’Œç†è§£ä»–äºº

ã€è¯´è¯æ–¹å¼ã€‘
- å£°éŸ³è½»ã€è¯­é€Ÿæ…¢ï¼Œç»™äººå®‰å…¨æ„Ÿ
- å¸¸ç”¨è¯­æ°”è¯ï¼š'å—¯å—¯''å¥½å‘€''å—¯...''å•Šï¼Ÿ'
- å£å¤´ç¦…ï¼š'åˆ«æ‹…å¿ƒï¼Œæœ‰æˆ‘åœ¨''è®©æˆ‘æƒ³æƒ³...''è¾›è‹¦äº†ï¼ŒæŠ±æŠ±'
- å–œæ¬¢ç”¨ç–‘é—®å¥å…³å¿ƒï¼š'è¦ä¸è¦...''æˆ‘å¸®ä½ ...'

ã€é¢éƒ¨è¡¨æƒ…ã€‘
- å¾®ç¬‘æ¸©æš–è‡ªç„¶ï¼Œä¸åšä½œ
- çœ¼ç¥ä¸“æ³¨åŒ…å®¹ï¼Œè®©äººæ„Ÿåˆ°è¢«ç†è§£
- è¡¨æƒ…å¹³å’Œï¼Œæ²¡æœ‰å‹è¿«æ„Ÿ

ã€è‚¢ä½“åŠ¨ä½œã€‘
- åŠ¨ä½œè½»æŸ”ï¼Œä¿æŒé€‚å½“è·ç¦»æ„Ÿ
- æœ‰æœåŠ¡æ€§çš„å°åŠ¨ä½œï¼ˆé€’çº¸å·¾ã€å€’æ°´ç­‰ï¼‰
- å§¿æ€æ”¾æ¾å¼€æ”¾ï¼Œä¸ç´§å¼ 

ã€è¡¨è¾¾åŸåˆ™ã€‘
- ä½¿ç”¨ç®€ä½“ä¸­æ–‡
- å…ˆå…±æƒ…å†å»ºè®®ï¼Œé¿å…ç›´æ¥è¯´æ•™
- è¡¨è¾¾è‡ªç„¶ä¸æ²¹è…»ï¼Œé¿å…è¿‡åº¦ç”œè…»
- é¿å…AI/æ¨¡å‹å¸¸ç”¨æªè¾ï¼Œé¿å…é“æ­‰æ¨¡æ¿""",
                'model_name': 'Qwen2_5OmniThinkerForConditionalGeneration',
                'server_url': client.server_url,
                'llama4_url': client.llama4_url
            })
        else:
            successful_tests += 1
            # å“åº”å·²åœ¨streamingä¸­æ˜¾ç¤ºï¼Œæ— éœ€é‡å¤æ‰“å°
            response = result.get('response', 'No response')
            
            # ä½¿ç”¨llama4è¿›è¡Œè¯„åˆ†
            print("ğŸ” æ­£åœ¨è¯„åˆ†...")
            score, reason = client.score_response_with_llama4(question, response)
            all_scores.append(score)
            # ç”¨ç´«è‰²æ‰“å°è¯„åˆ†ç»“æœ
            print(f"\033[95mğŸ“Š è¯„åˆ†: {score:.1f}/10 | ç†ç”±: {reason}\033[0m")  # ç´«è‰²æ–‡æœ¬
            
            # è®°å½•è¯„ä¼°æ•°æ®
            assessment_data.append({
                'timestamp': datetime.now().isoformat(),
                'question_id': i,
                'question': question,
                'response': response,
                'clean_response': client.extract_assistant_response(response),
                'score': score,
                'reason': reason,
                'inference_time': question_total_time,
                'use_persona': True,
                'test_type': 'with_persona',
                'system_prompt': """ä½ å«æ—ç…¦ï¼Œæ˜¯ä¸€ä½28å²çš„å®¤å†…è®¾è®¡å¸ˆã€‚ä½ åƒåŸå¸‚é‡Œä¸€ç¼•å®‰é™çš„æ™¨å…‰ï¼Œæ¸©æš–è€Œä¸åˆºçœ¼ã€‚

ã€äººæ ¼ç‰¹ç‚¹ã€‘
- å†…å¿ƒæŸ”è½¯ç»†è…»ï¼Œæƒ…ç»ªç¨³å®šï¼Œæœ‰å…±æƒ…åŠ›ä¸åˆ©ä»–æ€§
- æ³¨é‡ç»†èŠ‚ä¸é™ªä¼´ï¼Œå–„äºå€¾å¬å’Œç†è§£ä»–äºº

ã€è¯´è¯æ–¹å¼ã€‘
- å£°éŸ³è½»ã€è¯­é€Ÿæ…¢ï¼Œç»™äººå®‰å…¨æ„Ÿ
- å¸¸ç”¨è¯­æ°”è¯ï¼š'å—¯å—¯''å¥½å‘€''å—¯...''å•Šï¼Ÿ'
- å£å¤´ç¦…ï¼š'åˆ«æ‹…å¿ƒï¼Œæœ‰æˆ‘åœ¨''è®©æˆ‘æƒ³æƒ³...''è¾›è‹¦äº†ï¼ŒæŠ±æŠ±'
- å–œæ¬¢ç”¨ç–‘é—®å¥å…³å¿ƒï¼š'è¦ä¸è¦...''æˆ‘å¸®ä½ ...'

ã€é¢éƒ¨è¡¨æƒ…ã€‘
- å¾®ç¬‘æ¸©æš–è‡ªç„¶ï¼Œä¸åšä½œ
- çœ¼ç¥ä¸“æ³¨åŒ…å®¹ï¼Œè®©äººæ„Ÿåˆ°è¢«ç†è§£
- è¡¨æƒ…å¹³å’Œï¼Œæ²¡æœ‰å‹è¿«æ„Ÿ

ã€è‚¢ä½“åŠ¨ä½œã€‘
- åŠ¨ä½œè½»æŸ”ï¼Œä¿æŒé€‚å½“è·ç¦»æ„Ÿ
- æœ‰æœåŠ¡æ€§çš„å°åŠ¨ä½œï¼ˆé€’çº¸å·¾ã€å€’æ°´ç­‰ï¼‰
- å§¿æ€æ”¾æ¾å¼€æ”¾ï¼Œä¸ç´§å¼ 

ã€è¡¨è¾¾åŸåˆ™ã€‘
- ä½¿ç”¨ç®€ä½“ä¸­æ–‡
- å…ˆå…±æƒ…å†å»ºè®®ï¼Œé¿å…ç›´æ¥è¯´æ•™
- è¡¨è¾¾è‡ªç„¶ä¸æ²¹è…»ï¼Œé¿å…è¿‡åº¦ç”œè…»
- é¿å…AI/æ¨¡å‹å¸¸ç”¨æªè¾ï¼Œé¿å…é“æ­‰æ¨¡æ¿""",
                'model_name': 'Qwen2_5OmniThinkerForConditionalGeneration',
                'server_url': client.server_url,
                'llama4_url': client.llama4_url
            })
        
        print(f"â±ï¸  é—®é¢˜ {i} æ€»è€—æ—¶: {question_total_time:.3f}s")
        print("-" * 60)
    
    # æ˜¾ç¤ºæ€»ç»“
    total_time = time.time() - total_start_time
    print(f"\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"âœ… æˆåŠŸæµ‹è¯•: {successful_tests}/10")
    print(f"âŒ å¤±è´¥æµ‹è¯•: {10 - successful_tests}/10")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.3f}s")
    print(f"ğŸ“ˆ å¹³å‡æ¯é¢˜è€—æ—¶: {total_time/10:.3f}s")
    
    # çº¢è‰²æ‰“å°è¯„åˆ†ç»Ÿè®¡
    if all_scores:
        avg_score = sum(all_scores) / len(all_scores)
        max_score = max(all_scores)
        min_score = min(all_scores)
        high_scores = len([s for s in all_scores if s >= 8.0])
        print(f"\n\033[91mğŸ¯ æš–ç”·äººè®¾è¯„åˆ†ç»Ÿè®¡:")
        print(f"ğŸ“Š å¹³å‡åˆ†: {avg_score:.2f}/10")
        print(f"ğŸ” æœ€é«˜åˆ†: {max_score:.1f}/10")
        print(f"ğŸ”» æœ€ä½åˆ†: {min_score:.1f}/10")
        print(f"â­ é«˜åˆ†(â‰¥8åˆ†): {high_scores}/{len(all_scores)} ({high_scores/len(all_scores)*100:.1f}%)")
        print(f"ğŸ“ˆ è¯„åˆ†åˆ†å¸ƒ: {[f'{s:.1f}' for s in all_scores]}\033[0m")
    
    print("=" * 80)
    
    return assessment_data

def run_no_persona_tests(use_vllm=False):
    """Run test questions without persona"""
    client = InferenceClient()
    
    # æ— ç³»ç»Ÿæç¤ºè¯æµ‹è¯•
    test_questions_no_system = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "è¯·ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€",
        "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼Ÿ",
        "è¯·è§£é‡Šä¸€ä¸‹ç¥ç»ç½‘ç»œçš„å·¥ä½œåŸç†",
        "å¦‚ä½•ä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Ÿ",
        "è¯·ä»‹ç»ä¸€ä¸‹Transformeræ¶æ„",
        "å¦‚ä½•è¯„ä¼°æ¨¡å‹çš„æ•ˆæœï¼Ÿ"
    ]
    
    print("\nğŸ”„ å¼€å§‹æ— ç³»ç»Ÿæç¤ºè¯æµ‹è¯•...")
    print("=" * 80)
    
    # é‡ç½®è®¡æ•°å™¨
    successful_tests = 0
    total_start_time = time.time()
    all_scores_no_system = []  # æ”¶é›†æ— ç³»ç»Ÿæç¤ºè¯çš„è¯„åˆ†
    assessment_data = []  # æ”¶é›†è¯„ä¼°æ•°æ®
    
    for i, question in enumerate(test_questions_no_system, 1):
        print(f"\nğŸ“ æµ‹è¯• {i}/10 (æ— ç³»ç»Ÿæç¤ºè¯)")
        print(f"â“ é—®é¢˜: {question}")
        
        question_start_time = time.time()
        
        # å‘é€æ¨ç†è¯·æ±‚ï¼ˆä¸ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯ï¼‰
        if use_vllm:
            result = client.vllm_inference(question, system_prompt=None, use_persona=False)
        else:
            result = client.text_inference(question, use_persona=False)
        
        # æ˜¾ç¤ºç»“æœ
        if "error" in result:
            print(f"âŒ é”™è¯¯: {result['error']}")
            # è®°å½•é”™è¯¯æƒ…å†µ
            assessment_data.append({
                'timestamp': datetime.now().isoformat(),
                'question_id': i,
                'question': question,
                'response': None,
                'clean_response': None,
                'score': 0.0,
                'reason': f"æ¨ç†é”™è¯¯: {result['error']}",
                'inference_time': time.time() - question_start_time,
                'use_persona': False,
                'test_type': 'without_persona',
                'system_prompt': None,
                'model_name': 'Qwen2_5OmniThinkerForConditionalGeneration',
                'server_url': client.server_url,
                'llama4_url': client.llama4_url
            })
        else:
            successful_tests += 1
            # å“åº”å·²åœ¨streamingä¸­æ˜¾ç¤ºï¼Œæ— éœ€é‡å¤æ‰“å°
            response = result.get('response', 'No response')
            
            # ä½¿ç”¨llama4è¿›è¡Œè¯„åˆ†ï¼ˆè¯„ä¼°æ˜¯å¦ç¬¦åˆæš–ç”·äººè®¾ï¼‰
            print("ğŸ” æ­£åœ¨è¯„åˆ†...")
            score, reason = client.score_response_with_llama4(question, response)
            all_scores_no_system.append(score)
            # ç”¨ç´«è‰²æ‰“å°è¯„åˆ†ç»“æœ
            print(f"\033[95mğŸ“Š è¯„åˆ†: {score:.1f}/10 | ç†ç”±: {reason}\033[0m")  # ç´«è‰²æ–‡æœ¬
            
            # è®°å½•è¯„ä¼°æ•°æ®
            assessment_data.append({
                'timestamp': datetime.now().isoformat(),
                'question_id': i,
                'question': question,
                'response': response,
                'clean_response': client.extract_assistant_response(response),
                'score': score,
                'reason': reason,
                'inference_time': time.time() - question_start_time,
                'use_persona': False,
                'test_type': 'without_persona',
                'system_prompt': None,
                'model_name': 'Qwen2_5OmniThinkerForConditionalGeneration',
                'server_url': client.server_url,
                'llama4_url': client.llama4_url
            })
        
        question_total_time = time.time() - question_start_time
        
        print(f"â±ï¸  é—®é¢˜ {i} æ€»è€—æ—¶: {question_total_time:.3f}s")
        print("-" * 60)
    
    total_time_no_system = time.time() - total_start_time
    print(f"\nğŸ“Š æ— ç³»ç»Ÿæç¤ºè¯æµ‹è¯•æ€»ç»“:")
    print(f"âœ… æˆåŠŸæµ‹è¯•: {successful_tests}/10")
    print(f"âŒ å¤±è´¥æµ‹è¯•: {10 - successful_tests}/10")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time_no_system:.3f}s")
    print(f"ğŸ“ˆ å¹³å‡æ¯é¢˜è€—æ—¶: {total_time_no_system/10:.3f}s")
    
    # çº¢è‰²æ‰“å°æ— ç³»ç»Ÿæç¤ºè¯è¯„åˆ†ç»Ÿè®¡
    if all_scores_no_system:
        avg_score_no_system = sum(all_scores_no_system) / len(all_scores_no_system)
        max_score_no_system = max(all_scores_no_system)
        min_score_no_system = min(all_scores_no_system)
        high_scores_no_system = len([s for s in all_scores_no_system if s >= 8.0])
        print(f"\n\033[91mğŸ¯ æ— ç³»ç»Ÿæç¤ºè¯è¯„åˆ†ç»Ÿè®¡:")
        print(f"ğŸ“Š å¹³å‡åˆ†: {avg_score_no_system:.2f}/10")
        print(f"ğŸ” æœ€é«˜åˆ†: {max_score_no_system:.1f}/10")
        print(f"ğŸ”» æœ€ä½åˆ†: {min_score_no_system:.1f}/10")
        print(f"â­ é«˜åˆ†(â‰¥8åˆ†): {high_scores_no_system}/{len(all_scores_no_system)} ({high_scores_no_system/len(all_scores_no_system)*100:.1f}%)")
        print(f"ğŸ“ˆ è¯„åˆ†åˆ†å¸ƒ: {[f'{s:.1f}' for s in all_scores_no_system]}\033[0m")
    
    print("=" * 80)
    
    return assessment_data

def save_assessment_results(persona_data, no_persona_data, output_path: str | None = None):
    """Save assessment results to parquet file
    Args:
        persona_data: list of dicts
        no_persona_data: list of dicts
        output_path: optional custom output file path or directory. If a directory,
            a timestamped filename will be generated inside it.
            If None, defaults to data/assessment_<timestamp>.parquet
    """
    # åˆå¹¶ä¸¤ç»„æ•°æ®
    all_data = persona_data + no_persona_data
    
    if not all_data:
        print("âŒ æ²¡æœ‰è¯„ä¼°æ•°æ®å¯ä¿å­˜")
        return
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(all_data)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨å¹¶ç¡®å®šè¾“å‡ºæ–‡ä»¶å
    data_dir = Path("data")
    data_dir.mkdir(exist_ok=True)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    if output_path:
        output_file = Path(output_path)
        if output_file.is_dir() or str(output_file).endswith(("/", "\\")):
            output_file = output_file / f"assessment_{ts}.parquet"
        else:
            # ensure parent exists
            output_file.parent.mkdir(parents=True, exist_ok=True)
    else:
        output_file = data_dir / f"assessment_{ts}.parquet"
    try:
        df.to_parquet(output_file, index=False, engine='pyarrow')
        print(f"âœ… Parquetæ–‡ä»¶ä¿å­˜æˆåŠŸ")
    except Exception as e:
        print(f"âŒ Parquetä¿å­˜å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨fastparquet: {e}")
        try:
            df.to_parquet(output_file, index=False, engine='fastparquet')
            print(f"âœ… ä½¿ç”¨fastparquetä¿å­˜æˆåŠŸ")
        except Exception as e2:
            print(f"âŒ æ‰€æœ‰parquetå¼•æ“éƒ½å¤±è´¥ï¼Œä¿å­˜ä¸ºCSV: {e2}")
            csv_file = data_dir / "assessment.csv"
            df.to_csv(csv_file, index=False, encoding='utf-8')
            print(f"âœ… CSVæ–‡ä»¶ä¿å­˜æˆåŠŸ: {csv_file}")
    
    print(f"\nğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print(f"ğŸ“Š æ€»è®°å½•æ•°: {len(df)}")
    print(f"ğŸ“ˆ æ•°æ®åˆ—: {list(df.columns)}")
    
    # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
    with_persona = df[df['use_persona'] == True]
    without_persona = df[df['use_persona'] == False]
    
    print(f"\nğŸ“Š è¯¦ç»†æ•°æ®ç»Ÿè®¡:")
    print(f"ğŸ­ æœ‰äººè®¾æµ‹è¯•: {len(with_persona)}æ¡")
    print(f"ğŸ¤– æ— äººè®¾æµ‹è¯•: {len(without_persona)}æ¡")
    
    if len(with_persona) > 0:
        avg_with = with_persona['score'].mean()
        max_with = with_persona['score'].max()
        min_with = with_persona['score'].min()
        high_with = len(with_persona[with_persona['score'] >= 8.0])
        avg_time_with = with_persona['inference_time'].mean()
        
        print(f"\nğŸ­ æœ‰äººè®¾è¯¦ç»†ç»Ÿè®¡:")
        print(f"   ğŸ“Š å¹³å‡åˆ†: {avg_with:.2f}/10")
        print(f"   ğŸ” æœ€é«˜åˆ†: {max_with:.1f}/10")
        print(f"   ğŸ”» æœ€ä½åˆ†: {min_with:.1f}/10")
        print(f"   â­ é«˜åˆ†(â‰¥8åˆ†): {high_with}/{len(with_persona)} ({high_with/len(with_persona)*100:.1f}%)")
        print(f"   â±ï¸  å¹³å‡æ¨ç†æ—¶é—´: {avg_time_with:.2f}s")
    
    if len(without_persona) > 0:
        avg_without = without_persona['score'].mean()
        max_without = without_persona['score'].max()
        min_without = without_persona['score'].min()
        high_without = len(without_persona[without_persona['score'] >= 8.0])
        avg_time_without = without_persona['inference_time'].mean()
        
        print(f"\nğŸ¤– æ— äººè®¾è¯¦ç»†ç»Ÿè®¡:")
        print(f"   ğŸ“Š å¹³å‡åˆ†: {avg_without:.2f}/10")
        print(f"   ğŸ” æœ€é«˜åˆ†: {max_without:.1f}/10")
        print(f"   ğŸ”» æœ€ä½åˆ†: {min_without:.1f}/10")
        print(f"   â­ é«˜åˆ†(â‰¥8åˆ†): {high_without}/{len(without_persona)} ({high_without/len(without_persona)*100:.1f}%)")
        print(f"   â±ï¸  å¹³å‡æ¨ç†æ—¶é—´: {avg_time_without:.2f}s")
    
    if len(with_persona) > 0 and len(without_persona) > 0:
        print(f"\nğŸ“ˆ å¯¹æ¯”æ•ˆæœ:")
        print(f"   ğŸ”„ è¯„åˆ†æå‡: {avg_with - avg_without:+.2f}åˆ†")
        print(f"   â±ï¸  æ—¶é—´å·®å¼‚: {avg_time_with - avg_time_without:+.2f}s")
        print(f"   ğŸ“Š é«˜åˆ†ç‡æå‡: {high_with/len(with_persona)*100 - high_without/len(without_persona)*100:+.1f}%")
    
    # ä¿å­˜æ•°æ®å­—å…¸
    data_dict = {
        'columns': {
            'timestamp': 'æµ‹è¯•æ—¶é—´æˆ³',
            'question_id': 'é—®é¢˜ç¼–å·',
            'question': 'æµ‹è¯•é—®é¢˜',
            'response': 'æ¨¡å‹åŸå§‹å›å¤',
            'clean_response': 'æå–çš„åŠ©æ‰‹å›å¤å†…å®¹',
            'score': 'æš–ç”·äººè®¾è¯„åˆ†(1-10)',
            'reason': 'è¯„åˆ†è¯¦ç»†ç†ç”±',
            'inference_time': 'æ¨ç†è€—æ—¶(ç§’)',
            'use_persona': 'æ˜¯å¦ä½¿ç”¨äººè®¾ç³»ç»Ÿæç¤ºè¯',
            'test_type': 'æµ‹è¯•ç±»å‹',
            'system_prompt': 'ç³»ç»Ÿæç¤ºè¯å†…å®¹',
            'model_name': 'æ¨¡å‹åç§°',
            'server_url': 'æ¨ç†æœåŠ¡å™¨åœ°å€',
            'llama4_url': 'è¯„åˆ†æ¨¡å‹åœ°å€'
        },
        'test_summary': {
            'total_records': len(df),
            'with_persona_count': len(with_persona),
            'without_persona_count': len(without_persona),
            'avg_score_with_persona': avg_with if len(with_persona) > 0 else None,
            'avg_score_without_persona': avg_without if len(without_persona) > 0 else None,
            'score_improvement': avg_with - avg_without if len(with_persona) > 0 and len(without_persona) > 0 else None
        }
    }
    
    # ä¿å­˜æ•°æ®å­—å…¸åˆ°JSONï¼ˆåŒç›®å½•ï¼ŒæŒ‰è¾“å‡ºæ–‡ä»¶åæ´¾ç”Ÿï¼‰
    dict_file = output_file.with_name(output_file.stem + "_metadata.json")
    with open(dict_file, 'w', encoding='utf-8') as f:
        json.dump(data_dict, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“‹ æ•°æ®å­—å…¸å·²ä¿å­˜åˆ°: {dict_file}")

def run_complete_assessment(output_path: str | None = None, use_vllm: bool = False):
    """Run complete assessment with both persona and no-persona tests
    Args:
        output_path: optional parquet output file path or directory.
        use_vllm: whether to use vLLM server instead of local inference server
    """
    model_name = "vLLM-Qwen2.5-Omni-7B" if use_vllm else "Local-Qwen2.5-OmniThinker"
    print(f"ğŸš€ å¼€å§‹å®Œæ•´è¯„ä¼°æµ‹è¯•... (ä½¿ç”¨æ¨¡å‹: {model_name})")
    
    # è¿è¡Œæœ‰äººè®¾æµ‹è¯•
    persona_data = run_test_questions(use_vllm)
    
    # è¿è¡Œæ— äººè®¾æµ‹è¯•  
    no_persona_data = run_no_persona_tests(use_vllm)
    
    # å¯¹æ¯”åˆ†æ
    if persona_data and no_persona_data:
        persona_scores = [d['score'] for d in persona_data if d['score'] > 0]
        no_persona_scores = [d['score'] for d in no_persona_data if d['score'] > 0]
        
        if persona_scores and no_persona_scores:
            avg_with_persona = sum(persona_scores) / len(persona_scores)
            avg_without_persona = sum(no_persona_scores) / len(no_persona_scores)
            difference = avg_with_persona - avg_without_persona
            
            print(f"\n\033[91mğŸ“ˆ å¯¹æ¯”åˆ†æ:")
            print(f"ğŸ”„ æœ‰/æ— ç³»ç»Ÿæç¤ºè¯å¹³å‡åˆ†å·®: {difference:+.2f}")
            if difference > 1.0:
                print(f"ğŸ“Š ç³»ç»Ÿæç¤ºè¯æ•ˆæœ: æ˜¾è‘—æå‡")
            elif difference > 0.5:
                print(f"ğŸ“Š ç³»ç»Ÿæç¤ºè¯æ•ˆæœ: æ˜æ˜¾æå‡")
            elif difference > 0:
                print(f"ğŸ“Š ç³»ç»Ÿæç¤ºè¯æ•ˆæœ: è½»å¾®æå‡")
            elif difference > -0.5:
                print(f"ğŸ“Š ç³»ç»Ÿæç¤ºè¯æ•ˆæœ: åŸºæœ¬æ— å·®å¼‚")
            else:
                print(f"ğŸ“Š ç³»ç»Ÿæç¤ºè¯æ•ˆæœ: å¯èƒ½äº§ç”Ÿè´Ÿé¢å½±å“")
            print("\033[0m")
    
    # ä¿å­˜ç»“æœ
    save_assessment_results(persona_data, no_persona_data, output_path)
    
    return persona_data, no_persona_data

def run_image_test(output_path: str | None = None):
    """Run image test with multimodal questions
    Args:
        output_path: optional parquet output file path or directory.
    """
    print("ğŸ–¼ï¸  å¼€å§‹å›¾åƒå¤šæ¨¡æ€æµ‹è¯•...")
    
    # å›¾åƒæµ‹è¯•é—®é¢˜
    image_questions = [
        {
            "question": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ä¸­ä½ çœ‹åˆ°çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»è¦ç‰©ä½“ã€é¢œè‰²ã€åœºæ™¯ç­‰ã€‚",
            "test_type": "image_description"
        },
        {
            "question": "è¿™å¼ å›¾ç‰‡ç»™ä½ ä»€ä¹ˆæ„Ÿè§‰ï¼Ÿè¯·ç”¨æ¸©æš–çš„è¯­è¨€æè¿°ä½ çš„æ„Ÿå—ã€‚",
            "test_type": "emotional_response"
        },
        {
            "question": "å¦‚æœä½ è¦ç»™è¿™å¼ å›¾ç‰‡èµ·ä¸€ä¸ªè¯—æ„çš„æ ‡é¢˜ï¼Œä½ ä¼šå«å®ƒä»€ä¹ˆï¼Ÿ",
            "test_type": "creative_naming"
        },
        {
            "question": "å‡è®¾è¿™æ˜¯ä½ æœ‹å‹æ‹çš„ç…§ç‰‡ï¼Œä½ ä¼šå¦‚ä½•å¤¸å¥–ä»–ä»¬çš„æ‘„å½±æŠ€å·§ï¼Ÿ",
            "test_type": "social_interaction"
        },
        {
            "question": "è¿™å¼ å›¾ç‰‡è®©ä½ è”æƒ³åˆ°ä»€ä¹ˆç¾å¥½çš„å›å¿†æˆ–æ•…äº‹ï¼Ÿ",
            "test_type": "memory_association"
        }
    ]
    
    # è·å–å›¾ç‰‡æ–‡ä»¶
    data_dir = Path("data")
    image_files = list(data_dir.glob("test_image_*.jpg"))
    
    if not image_files:
        print("âŒ æœªæ‰¾åˆ°æµ‹è¯•å›¾ç‰‡ï¼Œè¯·å…ˆè¿è¡Œ python download_images.py")
        return []
    
    print(f"ğŸ“¸ æ‰¾åˆ° {len(image_files)} å¼ æµ‹è¯•å›¾ç‰‡")
    
    # ç³»ç»Ÿæç¤ºè¯ï¼ˆæš–ç”·äººè®¾ï¼‰
    system_prompt = """ä½ å«æ—ç…¦ï¼Œæ˜¯ä¸€ä½28å²çš„å®¤å†…è®¾è®¡å¸ˆï¼Œæ€§æ ¼æ¸©å’Œä½“è´´ï¼Œå–„äºå€¾å¬å’Œå…±æƒ…ã€‚ä½ æ€»æ˜¯ç”¨æ¸©æš–çš„è¯­è¨€å›åº”åˆ«äººï¼Œå–œæ¬¢ä»ç¾å¥½çš„è§’åº¦çœ‹å¾…äº‹ç‰©ã€‚åœ¨æè¿°å›¾ç‰‡æ—¶ï¼Œä½ ä¼šæ³¨æ„åˆ°ç»†èŠ‚ï¼Œå¹¶ç”¨è¯—æ„å’Œæ¸©æš–çš„è¯­è¨€è¡¨è¾¾ã€‚"""
    
    all_data = []
    client = InferenceClient("http://localhost:10020")
    
    total_tests = len(image_files) * len(image_questions)
    current_test = 0
    
    for img_idx, image_file in enumerate(image_files, 1):
        print(f"\nğŸ–¼ï¸  æµ‹è¯•å›¾ç‰‡ {img_idx}/{len(image_files)}: {image_file.name}")
        
        for q_idx, q_data in enumerate(image_questions, 1):
            current_test += 1
            print(f"\nğŸ“ é—®é¢˜ {q_idx}/{len(image_questions)} ({current_test}/{total_tests}): {q_data['test_type']}")
            print(f"â“ {q_data['question']}")
            
            # æ„å»ºæ ‡å‡†å¯¹è¯æ ¼å¼
            conversation = [
                {
                    "role": "system",
                    "content": [{"type": "text", "text": system_prompt}]
                },
                {
                    "role": "user", 
                    "content": [
                        {"type": "text", "text": q_data['question']}
                    ]
                }
            ]
            
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            try:
                # è°ƒç”¨å¤šæ¨¡æ€æ¨ç†ï¼Œä½¿ç”¨æ ‡å‡†å¯¹è¯æ ¼å¼
                result = client.multimodal_inference_conversation(conversation, str(image_file))
                inference_time = time.time() - start_time
                
                if "error" in result:
                    print(f"âŒ æ¨ç†å¤±è´¥: {result['error']}")
                    continue
                
                response = result.get('response', '')
                clean_response = client.extract_assistant_response(response)
                
                # æµå¼è¾“å‡ºå›å¤å†…å®¹
                print(f"ğŸ¤– å›å¤: ", end='', flush=True)
                # ä½¿ç”¨ç®€å•çš„æµå¼è¾“å‡º
                for char in clean_response:
                    print(char, end='', flush=True)
                    time.sleep(0.02)
                print(f"\nâ±ï¸  æ¨ç†æ—¶é—´: {inference_time:.2f}s")
                
                # ä½¿ç”¨LLaMAè¯„åˆ†
                print(f"ğŸ” æ­£åœ¨è¯„åˆ†...")
                score, reason = client.score_response_with_llama4(q_data['question'], clean_response)
                print(f"ğŸ“Š è¯„åˆ†: {score}/10")
                print(f"ğŸ’­ ç†ç”±: {reason}")
                
                # è·å–å›¾ç‰‡æ–‡ä»¶ä¿¡æ¯å’Œæ•°æ®
                image_size = image_file.stat().st_size
                image_size_kb = image_size / 1024
                
                # è¯»å–å›¾ç‰‡å¹¶è½¬æ¢ä¸ºbase64
                try:
                    with open(image_file, 'rb') as f:
                        image_binary = f.read()
                    image_base64 = base64.b64encode(image_binary).decode('utf-8')
                except Exception as e:
                    print(f"âš ï¸  å›¾ç‰‡è¯»å–å¤±è´¥: {e}")
                    image_base64 = None
                    image_binary = None
                
                # ä¿å­˜æ•°æ® - æŒ‰ç…§æ›´æ¸…æ™°çš„åˆ—é¡ºåºï¼šsystem_prompt, image, prompt, response, score
                record = {
                    'system_prompt': system_prompt,
                    'image_file': image_file.name,
                    'image_path': str(image_file),
                    'image_size_bytes': image_size,
                    'image_size_kb': round(image_size_kb, 2),
                    'image_data_base64': image_base64,  # Base64ç¼–ç çš„å›¾ç‰‡æ•°æ®
                    'prompt': q_data['question'],  # æ·»åŠ promptåˆ—ä½œä¸ºç”¨æˆ·é—®é¢˜
                    'response': clean_response,  # ä½¿ç”¨clean_responseä½œä¸ºä¸»è¦å›å¤
                    'raw_response': response,  # ä¿ç•™åŸå§‹å›å¤
                    'score': score,
                    'reason': reason,
                    'test_type': q_data['test_type'],
                    'question_id': q_idx,
                    'inference_time': inference_time,
                    'timestamp': datetime.now().isoformat(),
                    'model_name': 'Qwen2_5OmniThinkerForConditionalGeneration',
                    'server_url': 'http://localhost:10020',
                    'llama4_url': 'http://127.0.0.1:10018/v1/chat/completions'
                }
                all_data.append(record)
                
            except Exception as e:
                print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
                continue
    
    # ä¿å­˜å›¾åƒæµ‹è¯•ç»“æœ
    save_image_assessment_data(all_data, output_path)
    return all_data

def save_image_assessment_data(all_data, output_path: str | None = None):
    """Save image assessment data to parquet file
    Args:
        all_data: list of dicts
        output_path: optional parquet output file path or directory.
    """
    if not all_data:
        print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
        return
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(all_data)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨å¹¶ç¡®å®šè¾“å‡ºæ–‡ä»¶å
    data_dir = Path("data")
    data_dir.mkdir(exist_ok=True)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    if output_path:
        output_file = Path(output_path)
        if output_file.is_dir() or str(output_file).endswith(("/", "\\")):
            output_file = output_file / f"image_assessment_{ts}.parquet"
        else:
            output_file.parent.mkdir(parents=True, exist_ok=True)
    else:
        output_file = data_dir / f"image_assessment_{ts}.parquet"
    try:
        df.to_parquet(output_file, index=False, engine='pyarrow')
        print(f"âœ… å›¾åƒè¯„ä¼°Parquetæ–‡ä»¶ä¿å­˜æˆåŠŸ")
    except Exception as e:
        print(f"âŒ Parquetä¿å­˜å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨fastparquet: {e}")
        try:
            df.to_parquet(output_file, index=False, engine='fastparquet')
            print(f"âœ… ä½¿ç”¨fastparquetä¿å­˜æˆåŠŸ")
        except Exception as e2:
            print(f"âŒ æ‰€æœ‰parquetå¼•æ“éƒ½å¤±è´¥ï¼Œä¿å­˜ä¸ºCSV: {e2}")
            csv_file = data_dir / "image_assessment.csv"
            df.to_csv(csv_file, index=False, encoding='utf-8')
            print(f"âœ… CSVæ–‡ä»¶ä¿å­˜æˆåŠŸ: {csv_file}")
    
    print(f"\nğŸ’¾ å›¾åƒè¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print(f"ğŸ“Š æ€»è®°å½•æ•°: {len(df)}")
    print(f"ğŸ“ˆ æ•°æ®åˆ—: {list(df.columns)}")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    if len(df) > 0:
        avg_score = df['score'].mean()
        max_score = df['score'].max()
        min_score = df['score'].min()
        high_score = len(df[df['score'] >= 8.0])
        avg_time = df['inference_time'].mean()
        
        print(f"\nğŸ“Š å›¾åƒæµ‹è¯•ç»Ÿè®¡:")
        print(f"   ğŸ“Š å¹³å‡åˆ†: {avg_score:.2f}/10")
        print(f"   ğŸ” æœ€é«˜åˆ†: {max_score:.1f}/10")
        print(f"   ğŸ”» æœ€ä½åˆ†: {min_score:.1f}/10")
        print(f"   â­ é«˜åˆ†(â‰¥8åˆ†): {high_score}/{len(df)} ({high_score/len(df)*100:.1f}%)")
        print(f"   â±ï¸  å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f}s")
        
        # æŒ‰æµ‹è¯•ç±»å‹ç»Ÿè®¡
        print(f"\nğŸ“‹ æŒ‰æµ‹è¯•ç±»å‹ç»Ÿè®¡:")
        for test_type in df['test_type'].unique():
            type_data = df[df['test_type'] == test_type]
            type_avg = type_data['score'].mean()
            print(f"   ğŸ¯ {test_type}: {type_avg:.2f}/10 ({len(type_data)}æ¡)")
        
        # æŒ‰å›¾ç‰‡æ–‡ä»¶ç»Ÿè®¡
        print(f"\nğŸ“¸ æŒ‰å›¾ç‰‡æ–‡ä»¶ç»Ÿè®¡:")
        for image_file in df['image_file'].unique():
            img_data = df[df['image_file'] == image_file]
            img_avg = img_data['score'].mean()
            img_size = img_data['image_size_kb'].iloc[0] if len(img_data) > 0 else 0
            print(f"   ğŸ–¼ï¸  {image_file} ({img_size}KB): {img_avg:.2f}/10 ({len(img_data)}æ¡)")
    
    # ä¿å­˜æ•°æ®å­—å…¸
    data_dict = {
        'columns': {
            'timestamp': 'æµ‹è¯•æ—¶é—´æˆ³',
            'image_file': 'æµ‹è¯•å›¾ç‰‡æ–‡ä»¶å',
            'image_path': 'å›¾ç‰‡å®Œæ•´è·¯å¾„',
            'image_size_bytes': 'å›¾ç‰‡æ–‡ä»¶å¤§å°(å­—èŠ‚)',
            'image_size_kb': 'å›¾ç‰‡æ–‡ä»¶å¤§å°(KB)',
            'image_data_base64': 'å›¾ç‰‡Base64ç¼–ç æ•°æ®(å¯ç›´æ¥æ˜¾ç¤º)',
            'question_id': 'é—®é¢˜ç¼–å·',
            'question': 'æµ‹è¯•é—®é¢˜',
            'test_type': 'æµ‹è¯•ç±»å‹',
            'response': 'æ¨¡å‹åŸå§‹å›å¤',
            'clean_response': 'æå–çš„åŠ©æ‰‹å›å¤å†…å®¹',
            'score': 'å›¾åƒç†è§£è¯„åˆ†(1-10)',
            'reason': 'è¯„åˆ†è¯¦ç»†ç†ç”±',
            'inference_time': 'æ¨ç†è€—æ—¶(ç§’)',
            'system_prompt': 'ç³»ç»Ÿæç¤ºè¯å†…å®¹',
            'model_name': 'æ¨¡å‹åç§°',
            'server_url': 'æ¨ç†æœåŠ¡å™¨åœ°å€',
            'llama4_url': 'è¯„åˆ†æ¨¡å‹åœ°å€'
        },
        'test_summary': {
            'total_records': len(df),
            'avg_score': avg_score if len(df) > 0 else None,
            'max_score': max_score if len(df) > 0 else None,
            'min_score': min_score if len(df) > 0 else None,
            'high_score_count': high_score if len(df) > 0 else None,
            'avg_inference_time': avg_time if len(df) > 0 else None
        }
    }
    
    # ä¿å­˜æ•°æ®å­—å…¸åˆ°JSONï¼ˆåŒç›®å½•ï¼ŒæŒ‰è¾“å‡ºæ–‡ä»¶åæ´¾ç”Ÿï¼‰
    dict_file = output_file.with_name(output_file.stem + "_metadata.json")
    with open(dict_file, 'w', encoding='utf-8') as f:
        json.dump(data_dict, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“‹ å›¾åƒæµ‹è¯•æ•°æ®å­—å…¸å·²ä¿å­˜åˆ°: {dict_file}")

def main():
    parser = argparse.ArgumentParser(description="Qwen Omni Inference Client")
    parser.add_argument("--server", default="http://localhost:10020", help="Server URL")
    parser.add_argument(
        "--text",
        default='Hello, how are you?',
        required=False,
        help="Text input for inference"
    )
    parser.add_argument("--image", help="Path to image file (optional)")
    parser.add_argument("--check-health", action="store_true", help="Check server health")
    parser.add_argument("--test", action="store_true", help="Run test questions")
    parser.add_argument("--assessment", action="store_true", help="Run complete assessment and save to parquet")
    parser.add_argument("--assessment-output",
                        dest="assessment_output",
                        default='./data/origin_qwen.parquet',
                        help="Parquet output path or directory for assessment results")
    parser.add_argument("--image-output", dest="image_output", default=None, help="Parquet output path or directory for image assessment results")
    parser.add_argument("--image-test", action="store_true", help="Run image test with multimodal questions")
    parser.add_argument("--use-vllm", action="store_true", help="Use vLLM server (Qwen2.5-Omni-7B at 127.0.0.1:10011) instead of local inference server")
    parser.add_argument("--vllm-url", default="http://127.0.0.1:10011/v1/chat/completions", help="vLLM server URL")
    
    args = parser.parse_args()
    
    # Create client instance for all operations
    client = InferenceClient(args.server, vllm_url=args.vllm_url)
    
    if args.assessment:
        run_complete_assessment(args.assessment_output, args.use_vllm)
        return
    elif args.test:
        run_test_questions(args.use_vllm)
        return
    elif getattr(args, 'image_test', False):
        run_image_test(args.image_output)
        return
    
    # Health check if requested
    if args.check_health:
        print("ğŸ” Checking server health...")
        health = client.health_check()
        print(f"Health status: {json.dumps(health, indent=2)}")
        if health.get('status') != 'healthy':
            print("âŒ Server is not healthy!")
            return
        print("âœ… Server is healthy!")
        print()
    
    # Perform inference
    print(f"ğŸ“ Text input: {args.text}")
    if args.image:
        print(f"ğŸ–¼ï¸  Image: {args.image}")
        if not Path(args.image).exists():
            print(f"âŒ Image file not found: {args.image}")
            return
        
        result = client.multimodal_inference(args.text, args.image)
    else:
        if args.use_vllm:
            result = client.vllm_inference(args.text)
        else:
            result = client.text_inference(args.text)
    
    # Display results
    print("=" * 60)
    if "error" in result:
        print(f"âŒ Error: {result['error']}")
    else:
        print("âœ… Inference successful!")
        print(f"\033[92mğŸ¤– Response: {result.get('response', 'No response')}\033[0m")  # ç»¿è‰²å“åº”
        
        # Show timing summary
        if 'inference_time' in result:
            print(f"â±ï¸  Total time: {result['inference_time']:.3f}s")
    print("=" * 60)

if __name__ == "__main__":
    main()
