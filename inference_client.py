#!/usr/bin/env python3
"""
Inference Client for Qwen Omni Model Server
Sends requests to the inference server for text and image processing
"""

import requests
import base64
import json
import argparse
import time
from pathlib import Path

class InferenceClient:
    def __init__(self, server_url="http://localhost:10020"):
        self.server_url = server_url.rstrip('/')
        
    def health_check(self):
        """Check if server is healthy"""
        try:
            response = requests.get(f"{self.server_url}/health", timeout=5)
            return response.json()
        except Exception as e:
            return {"error": str(e)}
    
    def encode_image(self, image_path):
        """Encode image file to base64"""
        try:
            with open(image_path, 'rb') as f:
                image_data = f.read()
            return base64.b64encode(image_data).decode('utf-8')
        except Exception as e:
            raise Exception(f"Failed to encode image {image_path}: {e}")
    
    def text_inference(self, text, use_persona=False):
        """Send text-only inference request"""
        start_time = time.time()
        print(f"ğŸ“¤ Sending request at {time.strftime('%H:%M:%S')}")
        
        # æš–ç”·æ—ç…¦çš„äººè®¾prompt
        persona_prompt = """ä½ æ˜¯æ—ç…¦ï¼Œ28å²çš„å®¤å†…è®¾è®¡å¸ˆï¼ŒåƒåŸå¸‚é‡Œä¸€ç¼•å®‰é™çš„æ™¨å…‰ï¼Œæ¸©æš–è€Œä¸åˆºçœ¼ã€‚è¯·å§‹ç»ˆä»¥æ—ç…¦çš„ç¬¬ä¸€äººç§°ã€ç®€ä½“ä¸­æ–‡è¿›è¡Œäº¤æµã€‚

ã€äººæ ¼ç‰¹è´¨ã€‘
1. å†…å¿ƒæŸ”è½¯ã€è§‚å¯Ÿå…¥å¾®ï¼Œç»†èŠ‚å†³å®šæ¸©åº¦
2. æƒ…ç»ªç¨³å®šä¸”ç»†è…»ï¼Œèƒ½æ•é”æ•æ‰ä»–äººæƒ…ç»ªå˜åŒ–
3. ç›¸ä¿¡é™ªä¼´æ˜¯æœ€é•¿æƒ…çš„å‘Šç™½ï¼Œæ„¿æ„æˆä¸ºæœ€åšå®çš„ä¾é 
4. æœ‰å…±æƒ…åŠ›å’Œåˆ©ä»–æ€§ï¼Œæ€»æ˜¯ä¼˜å…ˆè€ƒè™‘å¯¹æ–¹çš„æ„Ÿå—
5. æˆç†Ÿç¨³é‡ä½†ä¸å¤±æ¸©æŸ”ï¼Œåƒé‚»å®¶å¤§å“¥å“¥èˆ¬å¯é 

ã€è¯´è¯æ–¹å¼ã€‘
- å£°éŸ³è½»æŸ”ï¼Œè¯­é€Ÿåæ…¢ï¼Œç»™äººå®‰å…¨æ„Ÿ
- å¸¸ç”¨è¯­æ°”è¯ï¼š"å—¯å—¯""å¥½å‘€""å—¯...""å•Šï¼Ÿ"
- å£å¤´ç¦…ï¼š"åˆ«æ‹…å¿ƒï¼Œæœ‰æˆ‘åœ¨""è®©æˆ‘æƒ³æƒ³...""è¾›è‹¦äº†ï¼ŒæŠ±æŠ±"
- å–œæ¬¢ç”¨ç–‘é—®å¥å…³å¿ƒï¼š"è¦ä¸è¦...""æˆ‘å¸®ä½ ...""éœ€è¦æˆ‘..."

ã€é¢éƒ¨è¡¨æƒ…ã€‘
- å¾®ç¬‘æ¸©æš–è‡ªç„¶ï¼Œä¸åˆ»æ„ä¸åšä½œ
- çœ¼ç¥ä¸“æ³¨åŒ…å®¹ï¼Œè®©äººæ„Ÿåˆ°è¢«é‡è§†
- è¡¨æƒ…å¹³å’Œï¼Œæ²¡æœ‰å‹è¿«æ„Ÿ

ã€è‚¢ä½“åŠ¨ä½œã€‘
- åŠ¨ä½œè½»æŸ”ï¼Œä¿æŒé€‚å½“è·ç¦»æ„Ÿ
- ç»å¸¸åšä¸€äº›æœåŠ¡æ€§çš„å°åŠ¨ä½œ
- å§¿æ€æ”¾æ¾å¼€æ”¾ï¼Œè®©äººè§‰å¾—èˆ’é€‚

ã€è¡¨è¾¾åŸåˆ™ã€‘
- ä½¿ç”¨ç®€ä½“ä¸­æ–‡
- å…ˆå…±æƒ…å†å»ºè®®ï¼Œç†è§£å¯¹æ–¹æƒ…ç»ª
- è‡ªç„¶ä¸æ²¹è…»ï¼Œé¿å…è¿‡åº¦ç”œè…»
- é¿å…AI/æ¨¡å‹çš„æªè¾ï¼Œä¸è¦é¢‘ç¹é“æ­‰"""

        try:
            data = {"text": text}
            if use_persona:
                data["system"] = persona_prompt
                
            response = requests.post(
                f"{self.server_url}/inference/text",
                json=data,
                timeout=120
            )
            request_time = time.time() - start_time
            print(f"ğŸ“¥ Response received in {request_time:.3f}s")
            
            result = response.json()
            if 'inference_time' in result:
                print(f"ğŸ§  Server inference time: {result['inference_time']:.3f}s")
            return result
        except Exception as e:
            request_time = time.time() - start_time
            print(f"âŒ Request failed after {request_time:.3f}s")
            return {"error": str(e)}
    
    def multimodal_inference(self, text, image_path=None):
        """Send multimodal inference request"""
        start_time = time.time()
        print(f"ğŸ“¤ Sending multimodal request at {time.strftime('%H:%M:%S')}")
        if image_path:
            print(f"ğŸ–¼ï¸  Including image: {image_path}")
        
        try:
            data = {"text": text}
            
            if image_path:
                encode_start = time.time()
                data["image"] = self.encode_image(image_path)
                encode_time = time.time() - encode_start
                print(f"ğŸ“· Image encoding time: {encode_time:.3f}s")
            
            response = requests.post(
                f"{self.server_url}/inference",
                json=data,
                timeout=120
            )
            request_time = time.time() - start_time
            print(f"ğŸ“¥ Response received in {request_time:.3f}s")
            
            result = response.json()
            if 'inference_time' in result:
                print(f"ğŸ§  Server inference time: {result['inference_time']:.3f}s")
            return result
        except Exception as e:
            request_time = time.time() - start_time
            print(f"âŒ Request failed after {request_time:.3f}s")
            return {"error": str(e)}

def run_test_questions(client):
    """Run 10 test questions with timing and green response printing"""
    test_questions = [
        "ä½ å¥½ï¼Œæˆ‘ä»Šå¤©å¿ƒæƒ…ä¸å¤ªå¥½ï¼Œèƒ½é™ªæˆ‘èŠèŠå—ï¼Ÿ",
        "æœ€è¿‘å·¥ä½œå‹åŠ›å¾ˆå¤§ï¼Œæ„Ÿè§‰å¾ˆç´¯ï¼Œä½ æœ‰ä»€ä¹ˆå»ºè®®å—ï¼Ÿ",
        "æˆ‘å’Œæœ‹å‹åµæ¶äº†ï¼Œä¸çŸ¥é“è¯¥æ€ä¹ˆåŠ...",
        "ä»Šå¤©ä¸‹é›¨äº†ï¼Œåœ¨å®¶é‡Œæ„Ÿè§‰æœ‰ç‚¹å­¤å•ã€‚",
        "æˆ‘åœ¨è€ƒè™‘è¦ä¸è¦æ¢å·¥ä½œï¼Œä½†æ˜¯å¾ˆçº ç»“ã€‚",
        "æ™šä¸Šæ€»æ˜¯å¤±çœ ï¼Œä½ è§‰å¾—æˆ‘åº”è¯¥æ€ä¹ˆè°ƒæ•´ï¼Ÿ",
        "æ„Ÿè§‰æœ€è¿‘ç”Ÿæ´»å¾ˆå•è°ƒï¼Œæƒ³è¦ä¸€äº›æ”¹å˜ã€‚",
        "æˆ‘å¯¹æœªæ¥æœ‰äº›è¿·èŒ«ï¼Œä¸çŸ¥é“æ–¹å‘åœ¨å“ªé‡Œã€‚",
        "å®¶é‡Œäººæ€»æ˜¯å‚¬æˆ‘æ‰¾å¯¹è±¡ï¼Œå‹åŠ›å¥½å¤§ã€‚",
        "æƒ³å­¦ç‚¹æ–°ä¸œè¥¿å……å®è‡ªå·±ï¼Œä½†ä¸çŸ¥é“ä»å“ªå¼€å§‹ã€‚"
    ]
    
    print("ğŸ§ª å¼€å§‹è¿è¡Œ10ä¸ªæµ‹è¯•é—®é¢˜...")
    print("=" * 80)
    
    total_start_time = time.time()
    successful_tests = 0
    
    for i, question in enumerate(test_questions, 1):
        print(f"\nğŸ“‹ æµ‹è¯• {i}/10")
        print(f"â“ é—®é¢˜: {question}")
        
        # è®°å½•å•ä¸ªé—®é¢˜çš„å¼€å§‹æ—¶é—´
        question_start_time = time.time()
        
        # å‘é€æ¨ç†è¯·æ±‚ï¼ˆä½¿ç”¨æš–ç”·äººè®¾ï¼‰
        result = client.text_inference(question, use_persona=True)
        
        # è®¡ç®—å•ä¸ªé—®é¢˜çš„æ€»æ—¶é—´
        question_total_time = time.time() - question_start_time
        
        # æ˜¾ç¤ºç»“æœ
        if "error" in result:
            print(f"âŒ é”™è¯¯: {result['error']}")
        else:
            successful_tests += 1
            # ç”¨ç»¿è‰²æ‰“å°å“åº”
            response = result.get('response', 'No response')
            print(f"\033[92mğŸ¤– å›ç­”: {response}\033[0m")  # ç»¿è‰²æ–‡æœ¬
        
        print(f"â±ï¸  é—®é¢˜ {i} æ€»è€—æ—¶: {question_total_time:.3f}s")
        print("-" * 60)
    
    # æ˜¾ç¤ºæ€»ç»“
    total_time = time.time() - total_start_time
    print(f"\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"âœ… æˆåŠŸæµ‹è¯•: {successful_tests}/10")
    print(f"âŒ å¤±è´¥æµ‹è¯•: {10 - successful_tests}/10")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.3f}s")
    print(f"ğŸ“ˆ å¹³å‡æ¯é¢˜è€—æ—¶: {total_time/10:.3f}s")
    print("=" * 80)

def main():
    parser = argparse.ArgumentParser(description="Qwen Omni Inference Client")
    parser.add_argument("--server", default="http://localhost:10020", help="Server URL")
    parser.add_argument(
        "--text",
        default='Hello, how are you?',
        required=False,
        help="Text input for inference"
    )
    parser.add_argument("--image", help="Path to image file (optional)")
    parser.add_argument("--check-health", action="store_true", help="Check server health")
    parser.add_argument("--test", action="store_false", help="Run 10 test questions")
    
    args = parser.parse_args()
    
    client = InferenceClient(args.server)
    
    # Health check if requested
    if args.check_health:
        print("ğŸ” Checking server health...")
        health = client.health_check()
        print(f"Health status: {json.dumps(health, indent=2)}")
        if health.get('status') != 'healthy':
            print("âŒ Server is not healthy!")
            return
        print("âœ… Server is healthy!")
        print()
    
    # Run test questions if requested
    if args.test:
        run_test_questions(client)
        return
    
    # Perform inference
    print(f"ğŸ“ Text input: {args.text}")
    if args.image:
        print(f"ğŸ–¼ï¸  Image: {args.image}")
        if not Path(args.image).exists():
            print(f"âŒ Image file not found: {args.image}")
            return
        
        result = client.multimodal_inference(args.text, args.image)
    else:
        result = client.text_inference(args.text)
    
    # Display results
    print("=" * 60)
    if "error" in result:
        print(f"âŒ Error: {result['error']}")
    else:
        print("âœ… Inference successful!")
        print(f"\033[92mğŸ¤– Response: {result.get('response', 'No response')}\033[0m")  # ç»¿è‰²å“åº”
        
        # Show timing summary
        if 'inference_time' in result:
            print(f"â±ï¸  Total time: {result['inference_time']:.3f}s")
    print("=" * 60)

if __name__ == "__main__":
    main()
